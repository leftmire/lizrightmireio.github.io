[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nMy name is Liz Rightmire\nI’m a computer science student at Middlebury College\nThis blog holds the work I’ve done in CSCI0451: Machine Learning"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Liz’s CSCI 0451 Blog",
    "section": "",
    "text": "Whose Costs?\n\n\n\n\n\nDesigning and Evaluating a Profit-Maximizing Loan Default Predictor Model\n\n\n\n\n\nMar 1, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nSelecting features and model for accurate penguin species classifier\n\n\n\n\n\nFeb 20, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nGoal Setting\n\n\n\n\n\nGoal setting for beginning of semester\n\n\n\n\n\nJan 10, 2023\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ClassifyingPalmerPenguins/index.html",
    "href": "posts/ClassifyingPalmerPenguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "CSCI 0451\n\nClassifying Palmer Penguins\n\n\n\ncite: https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\n\n\nThe Palmer Penguins dataset is a public dataset frequently used within the educational data science community. It contains Dr. Kristen Gorman and the Long Term Ecological Research Network’s observations of hundreds of antartic penguins belonging to 3 species groups: Adelie, Gentoo, and Chinstrap.\nThis blog aims to accomplish three goals:\n\nConduct exploration of the Palmer Penguins dataset\nMethodically select 3 features and a model type that produces 100% penguin species classification accuracy on test data\nEvaluate chosen model by analyzing decision regions and a confusion matrix\n\n\nRandom Exploration\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nThe training data contains 18 observations about 275 penguin subjects. Let’s clean this data by dropping irrelevent columns and one-hot encoding the qualitative observations.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\nVisualizations\n\nimport seaborn as sns\nsns.set_palette(\"husl\", 3)\n\n# visualization 1: flipper length frequency by species\nsns.histplot(data = train, x = \"Flipper Length (mm)\", hue = \"Species\", bins = 25, edgecolor = 'grey')\n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis histogram shows the distribution of the penguins’ flipper lengths. The bars are colored based on penguin species, so the shape of the distributions allow the viewer to infer the mean, medians, and modes for each species. There is a trend in flipper length based on species: Gentoo penguins tend to have the largest flippers, and Adelie and Chinstrap penguins have smaller ones. This indicates that flipper length may be a helpful feature to use in classification, as it effectively identifies the Gentoo penguins from the rest of the species.\n\n# visualization 2: body mass vs. culmen length\nplot1 = sns.scatterplot(train, x = \"Body Mass (g)\", y = \"Culmen Length (mm)\", hue = \"Species\")\n\n\n\n\n\n\n\n\nAfter considering multiple combinations of qualitative features in scatterplots, body mass and culmen length proved to show clear groupings of penguin species. Therefore, these two features are likely to be effective in a classification model. That being said, overlap does exist, especially between Adelie and Gentoo penguins.\n\n# summary table\ntable = train.groupby(['Species', 'Island']).size()\ntable\n\nSpecies    Island   \nAdelie     Biscoe       33\n           Dream        45\n           Torgersen    42\nChinstrap  Dream        57\nGentoo     Biscoe       98\ndtype: int64\n\n\nThis summary table shows the numbers of penguins present on each island. Every penguin on Torgersen island is an Adelie penguin, and both Gentoo and Chinstrap penguins can only be found on one island. These clear trends cause me to consider island location as a potential feature in a classification model.\n\n\n\ncite: https://en.ac-illust.com/clip-art/22518802/illustration-of-a-cute-penguins-playing-a-computer\n\n\n\n\nChoosing Features\nBecause there were only 5 qualitative and 6 quantitative feature in the cleaned dataset, I chose to perform an exhaustive search to determine the most effective features and model for species classification. For each combination of 2 quantitative and 1 qualitative feature, I fit 4 models: Logistic Regression, Decision Tree, Random Forest, and SVM. Cross validation was performed to guard against overfitting as follows:\n\nLogistic Regression: recorded average accuracy of 5 rounds, each round with a random 20% of data used for testing\nDecision Tree and Random Forest: performed grid search of varrying max_depth values, recorded highest accuracy achieved\nSVM: performed grid search of varrying gamma values, recorded highest accuracy achieved\n\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('max_colwidth', 10000)\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)' ]\n\ncolumns = ['features', 'model', 'score']\n\nscore_df = pd.DataFrame(columns = columns)\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n\n    # Logistic Regression\n    LR = LogisticRegression(max_iter = 20000000000)\n    LR.fit(X_train[cols], y_train)  \n    LRscore = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n    score_df.loc[len(score_df.index)] = [cols, 'Logistic', LRscore]  \n\n    # Decision Tree\n    DTC = DecisionTreeClassifier()\n    param_grid = {'max_depth': [11,2,3,4,5,6,7,8,9,10, None]}\n    grid_search = GridSearchCV(DTC, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCscore = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, 'Decision Tree', DTCscore]  \n\n    # Random Forest\n    randomforest = RandomForestClassifier()\n    grid_search = GridSearchCV(randomforest, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    RF_score = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, 'Random Forest', RF_score]\n\n    # SVM\n    param_grid = {'gamma': 10.0**np.arange(-5, 5)}\n    SVC_model = SVC()\n    grid_search = GridSearchCV(SVC_model, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    SVMscore = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, \"SVM\", SVMscore]\n\nscore_df.sort_values(by='score', ascending=False).head(10)\n\n\n\n\n\n\n\n\nfeatures\nmodel\nscore\n\n\n\n\n60\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nLogistic\n0.988311\n\n\n62\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nRandom Forest\n0.988311\n\n\n66\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\nRandom Forest\n0.984465\n\n\n126\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Flipper Length (mm)]\nRandom Forest\n0.984389\n\n\n122\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\nRandom Forest\n0.984389\n\n\n120\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\nLogistic\n0.984389\n\n\n166\n[Island_Biscoe, Island_Dream, Island_Torgersen, Flipper Length (mm), Delta 13 C (o/oo)]\nRandom Forest\n0.984314\n\n\n138\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Delta 13 C (o/oo)]\nRandom Forest\n0.980543\n\n\n63\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nSVM\n0.980543\n\n\n134\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Delta 15 N (o/oo)]\nRandom Forest\n0.976621\n\n\n\n\n\n\n\nFor each model, the features, model type, and highest possible score was added to a dataframe. Sorting this dataframe by score revealed that Logistic Regression with Sex, Culmen Length and Culmen Depth as features produced the best classification.\n\n\n\ncitation: https://www.dreamstime.com/stock-illustration-d-penguin-teaches-math-render-numbers-image45736629\n\n\n\n\nEvaluate Chosen Model\nTo truly evaluate our model, we must evaluate how it performs on unseen testing data.\n\n# train with optimal features and model\nLR = LogisticRegression()\noptimal_features = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\nLR.fit(X_train[optimal_features], y_train)  \n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n#test\nX_test, y_test = prepare_data(test)\nLR.score(X_test[optimal_features], y_test)\n\n1.0\n\n\nA score of 1.0 indicates that 100% of the penguins in the testing dataset were correctly classified by our model. Yippie!\nStepping back a little, let’s consider: does this make sense? Should we be able to determine a penguin species based on its sex, culmen length, and culmen depth?\nYes, it seems logical that different species of penguins have different culmen dimensions. I am also not surprised that sex is an important qualitative feature to consider, as penguin size, and consequently beak dimensions, vary based on sex. For example, female Gentoo penguins may have similar culmen lengths to a male Adelie, so sex is required to determine species.\nTo be sure, let’s create a scatterplot of these 3 features and look at the decision regions produced by our logistic algorithm.\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the training data:\n\n# training data\nplot_regions(LR, X_train[optimal_features], y_train)\n\n\n\n\n\n\n\n\nFor the testing data:\n\n# testing data\nplot_regions(LR, X_test[optimal_features], y_test)\n\n\n\n\n\n\n\n\nYes, separating penguins by sex creates very clear clusters of points by species in the testing data, which transfers perfectly to the points in the testing data.\nAnother way to evaluate would be to look at a confusion matrix for this model\n\nfrom sklearn.metrics import confusion_matrix\n\n#actual\n#predicted\n\ny_test_pred = LR.predict(X_test[optimal_features]) # guesses for each data\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {cm[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\nZero penguins were mis-classified. This makes sense because we got our logistic regression model had an accuracy of 1.0!\nThe plot below is another way to visualize the confusion matrix. For an accuracy of 1.0, we would expect zeros in every box execept for those on the diagonal, meaning that zero penguins were misclassified.\n\n# plot confusion matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ncm_df = pd.DataFrame((cm), index = ['Gentoo','Chinstrap ','Adelie'], columns = ['Gentoo','Chinstrap','Adelie'])\ndisplay_labels = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nConfusionMatrixDisplay(cm, display_labels = display_labels).plot(\n    include_values=True)"
  },
  {
    "objectID": "posts/Goal-Setting/goal-setting.html",
    "href": "posts/Goal-Setting/goal-setting.html",
    "title": "Goal Setting",
    "section": "",
    "text": "Liz Rightmire\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs I progress through the computer science major, I have developed a specific interest in data science and machine learning – it’s what I hope to do postgrad. I’ve been looking forward to this class for a long time, and when looking at my schedule, it seems within the realm of possibility to make this class a priority. I have a bit of experience with with implementation and experimentation of ML through a data science internship last summer, but I am far from an expert :). Therefore, I hope to apply most of my focus to theory (I want to better understand HOW these algorithms work and leverage my mathematical experience!) and Social Responsibliity (with an aspired career in data science, I am concerned about my impact and I want to take advantage of my liberal arts education to understand the harm that can be done in this industry.)\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI hope to do most of the blog posts. I tend to work at a slower rate than the average student, and considering that they take 5-8 hours to complete, I want to leave room in my goal of 10hrs/week of ML for readings and warm-ups. I see myself learning a lot from the blog posts, so I want to do as many as possible without over-extending myself. I find it challenging to quantify a specific number of blog posts at this time. Instead, it feels more reasonable to say that knowing that we’ll have check-ins throughout the semester with the average numbers completed by the class, I hope my statistic will place me in an ambitous range.\nWhen I need to prioritize which blog posts to complete, I want to cover all categories but place special emphasis on Theory and Social Responsibility. I hope to propose and complete one additional blog post on Social Responsibility. Knowing myself, I don’t think it will be challenging to submit my work by the best-by date and I want to give myself flexibility to invest extra time into blog posts that particularly interest me!\nI believe that completing revisions is where I’ll learn the most, so I want to revise most (5?) of my blog posts to “no revisions suggested.”\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI aspire to do all of the readings – including some optional theory readings – as well as all of the warm-ups to the best of my ability. It doesn’t feel productive to place a maximum number of “passes” because I trust my effort and intentions in doing my best work; instead, I’ll try to attend student hours if I get stumped. I might need help from classmates sometimes, and I think that’s ok.\nBefore class, I aspire to preview the in-class coding we’ll be doing so that I prep myself well for understanding the content.\nI have already begun working with some classmates outside of class, so I hope to sometimes be the one to organize work times.\nSomething I want to work on is my presentation skills. I presented on an algorithm for my J-Term class, and I felt poorly about how it went. Each time I’m chosen to present the warm-up in ML, I want to treat it as practice speaking in front of others and engaging my peers in discussion. I’m not one to speak often in class, but I aspire to ask a few questions over the semester and maybe even answer some questions. More practical for me, perhaps, is to challenge myself to attend student hours to ask specific questions when I get stuck.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI look forward to working on a culmianting project with some classmates! Right now, I think I’ll be interested in finding a complex dataset and applying some of the techniques we’ve learned in class as well as trying some new techniques. At the end of the project, I want to feel as though I “pulled my weight” with technical contributions, but also practice my interpersonal teamwork skills and hold teammates accountable for work. The final presentation will be a great opportunity for me to apply what I’ve been practicing in leading warm-ups."
  },
  {
    "objectID": "posts/Goal-Setting/goal-setting.html#what-youll-learn",
    "href": "posts/Goal-Setting/goal-setting.html#what-youll-learn",
    "title": "Goal Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs I progress through the computer science major, I have developed a specific interest in data science and machine learning – it’s what I hope to do postgrad. I’ve been looking forward to this class for a long time, and when looking at my schedule, it seems within the realm of possibility to make this class a priority. I have a bit of experience with with implementation and experimentation of ML through a data science internship last summer, but I am far from an expert :). Therefore, I hope to apply most of my focus to theory (I want to better understand HOW these algorithms work and leverage my mathematical experience!) and Social Responsibliity (with an aspired career in data science, I am concerned about my impact and I want to take advantage of my liberal arts education to understand the harm that can be done in this industry.)"
  },
  {
    "objectID": "posts/Goal-Setting/goal-setting.html#what-youll-achieve",
    "href": "posts/Goal-Setting/goal-setting.html#what-youll-achieve",
    "title": "Goal Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI hope to do most of the blog posts. I tend to work at a slower rate than the average student, and considering that they take 5-8 hours to complete, I want to leave room in my goal of 10hrs/week of ML for readings and warm-ups. I see myself learning a lot from the blog posts, so I want to do as many as possible without over-extending myself. I find it challenging to quantify a specific number of blog posts at this time. Instead, it feels more reasonable to say that knowing that we’ll have check-ins throughout the semester with the average numbers completed by the class, I hope my statistic will place me in an ambitous range.\nWhen I need to prioritize which blog posts to complete, I want to cover all categories but place special emphasis on Theory and Social Responsibility. I hope to propose and complete one additional blog post on Social Responsibility. Knowing myself, I don’t think it will be challenging to submit my work by the best-by date and I want to give myself flexibility to invest extra time into blog posts that particularly interest me!\nI believe that completing revisions is where I’ll learn the most, so I want to revise most (5?) of my blog posts to “no revisions suggested.”\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI aspire to do all of the readings – including some optional theory readings – as well as all of the warm-ups to the best of my ability. It doesn’t feel productive to place a maximum number of “passes” because I trust my effort and intentions in doing my best work; instead, I’ll try to attend student hours if I get stumped. I might need help from classmates sometimes, and I think that’s ok.\nBefore class, I aspire to preview the in-class coding we’ll be doing so that I prep myself well for understanding the content.\nI have already begun working with some classmates outside of class, so I hope to sometimes be the one to organize work times.\nSomething I want to work on is my presentation skills. I presented on an algorithm for my J-Term class, and I felt poorly about how it went. Each time I’m chosen to present the warm-up in ML, I want to treat it as practice speaking in front of others and engaging my peers in discussion. I’m not one to speak often in class, but I aspire to ask a few questions over the semester and maybe even answer some questions. More practical for me, perhaps, is to challenge myself to attend student hours to ask specific questions when I get stuck.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI look forward to working on a culmianting project with some classmates! Right now, I think I’ll be interested in finding a complex dataset and applying some of the techniques we’ve learned in class as well as trying some new techniques. At the end of the project, I want to feel as though I “pulled my weight” with technical contributions, but also practice my interpersonal teamwork skills and hold teammates accountable for work. The final presentation will be a great opportunity for me to apply what I’ve been practicing in leading warm-ups."
  },
  {
    "objectID": "posts/WhoseCosts/index.html",
    "href": "posts/WhoseCosts/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "Whose Costs?\n\nIntroduction\nThis blog tackles a real world problem: given information on an individual and their specific loan request, should a bank offer this person a loan? I will conduct an exploration of the data to determine which features in the dataset are the best predictors of a person’s loan status (whether they will default on their loan, or not). Using logistic regression, I will determine a weight vector to use in a linear score function. Then, with the only goal of maximizing profit, I will choose a threshold value: individuals with scores below this value will be offered a loan by the mythical Liz Bank, and those with scores above this value will be denied. However, a model created with the sole goal of maximizing profit is bound to make harsh judgements and marginialize groups. At the end of this post, I’ll evaluate to what extent this is true.\n\n# import relevent packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nFirst, let’s grab the data. Our dataset has 12 features and 22,907 entries. Our predictor variable is loan_status: a 1 indicates that this person defaulted on the loan and the bank lost money (bad!), and a 0 means they didn’t default.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train = df_train.dropna()\ndf_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n14.91\n1\n0.25\nN\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n22907 rows × 12 columns\n\n\n\n\n\nExplore Data\nLet’s make a summary table and 2 visualizations to explore our data. In this process, we want to begin to determine which features may be good predictors of loan status.\nFirst, let’s look at the mean loan amount for different home ownership statuses. Unsurprisingly, people with mortgages had a high mean loan amount of ~10,600. This makes sense, as their loan may be the mortgage itself, or these individuals’ lack of disposable income may lead them to take loans for other purposes as well. People who rent and own houses had lower mean loan amounts around $8,000 - $9,000.\n\ntable = df_train.groupby(['person_home_ownership']).agg(\"loan_amnt\").mean().reset_index()\ntable\n\n\n\n\n\n\n\n\nperson_home_ownership\nloan_amnt\n\n\n\n\n0\nMORTGAGE\n10620.177719\n\n\n1\nOTHER\n11305.194805\n\n\n2\nOWN\n9051.955782\n\n\n3\nRENT\n8912.191822\n\n\n\n\n\n\n\nNext, I made a scatterplot illustrating loan status depending on loan interest rate and loan amount. From the graph, I was able to infer a decision boundary for loan interest rate: &gt;12.5% and people tend to default on their loans, &lt;12.5% and people tend not to. Curiously, it was challenging to see a pattern when considering loan amount.\n\nsns.set_palette(\"dark\")\nplot1 = sns.scatterplot(df_train, \n                        x = \"loan_int_rate\", \n                        y = \"loan_amnt\", \n                        hue = \"cb_person_default_on_file\", \n                        size = 0.5).set(xlabel = \" Loan Interest Rate \",\n                                        ylabel = \" Loan Amount\",\n                                        title = \"Defaults based on Loan Interest Rate and Loan Amount\")\n\nplt.legend(title = \"Default on File\")\n\nplot1\n\n[Text(0.5, 0, ' Loan Interest Rate '),\n Text(0, 0.5, ' Loan Amount'),\n Text(0.5, 1.0, 'Defaults based on Loan Interest Rate and Loan Amount')]\n\n\n\n\n\n\n\n\n\nFinally, these histograms helped me understand how purpose of loans change with age. Unsurprisingly, people in their twenties tend to take out loans for education, with people in their late twenties making venture loans to start businesses. With age, loans transition for medical and personal purposes.\n\nfig, axs = plt.subplots(3,2)\nplt.title(\"Distribution of Loan Intent by Age\")\nplt.subplots_adjust(hspace = 0.9)\n\nsns.histplot(data = df_train[df_train['loan_intent'] == \"VENTURE\"], x = \"person_age\", color=\"skyblue\", ax = axs[0,0], bins = 60).set(title = \"Venture\", ylim = (0,2000), xlim = (20,60), xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"EDUCATION\"], x = \"person_age\", color=\"olive\", ax = axs[1,0], bins = 60).set(title = \"Education\", ylim = (0,2000), xlim = (20,60), xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"MEDICAL\"], x = \"person_age\", color=\"gold\", ax = axs[2,0], bins = 35).set(title = \"Medical\", ylim = (0,2000), xlim = (20,60), xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"HOMEIMPROVEMENT\"], x = \"person_age\", ax = axs[0,1], bins = 25).set(title = \"Home Improvement\", ylim = (0,2000), xlim = (20,60), ylabel = None, xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"PERSONAL\"], x = \"person_age\", color=\"teal\", ax = axs[1,1], bins = 60).set(title = \"Personal\", ylim = (0,2000), xlim = (20,60), ylabel = None, xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"DEBTCONSOLIDATION\"], x = \"person_age\", ax = axs[2,1], bins = 25).set(title = \"Debt Consolidation\", ylim = (0,2000), xlim = (20,60), ylabel = None, xlabel = \"age\")\n\nfig.suptitle(' Distribution of Loan Intent by Age ', fontsize=20)\nplt.show()\n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n\nBuild a Model\nNow it’s time to build a model. First, one-hot-encode the qualitative columns in the training set and drop features not permitted in the mdoel (loan status – the predictor, and loan grade)\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# before one-hot-encoding, make copy of data for later visualization\ndf_for_viz = df_train.copy()\n\n# label-encode qualitative features\nle = LabelEncoder()\nfor label in [\"loan_intent\", \"cb_person_default_on_file\"]:\n    df_train[label] = le.fit_transform(df_train[label])\n\n# manually one-hot encode to force more logical order\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OTHER', 0)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('RENT', 1)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('MORTGAGE', 2)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OWN', 3)\n\ny_train = df_train[\"loan_status\"]\nX_train = df_train.drop(columns=['loan_status', 'loan_grade'])\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\n1\n3.0\n1\n11750\n13.47\n0.12\n1\n6\n\n\n2\n22\n36996\n1\n5.0\n1\n10000\n7.51\n0.27\n0\n4\n\n\n3\n24\n26000\n1\n2.0\n3\n1325\n12.87\n0.05\n0\n4\n\n\n4\n29\n53004\n2\n2.0\n2\n15000\n9.63\n0.28\n0\n10\n\n\n6\n21\n21700\n1\n2.0\n2\n5500\n14.91\n0.25\n0\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n2\n8.0\n1\n3000\n7.29\n0.02\n0\n17\n\n\n26060\n23\n48000\n1\n1.0\n5\n4325\n5.42\n0.09\n0\n4\n\n\n26061\n22\n60000\n1\n0.0\n3\n15000\n11.71\n0.25\n0\n4\n\n\n26062\n30\n144000\n2\n12.0\n4\n35000\n12.68\n0.24\n0\n8\n\n\n26063\n25\n60000\n1\n5.0\n1\n21450\n7.29\n0.36\n0\n4\n\n\n\n\n22907 rows × 10 columns\n\n\n\nWhat number of features, and which ones, are most effective in a model to predict loan status? To answer this question, I turned to logistic regression. Utilizing the combinations function from itertools, I fit a logistic regression model with 5 cross-validation batches for each combination of features. I began with NUMFEATURES = 2 to test couples of features and found that person_home_ownership and loan_percent_income produced the highest score of 0.849. Adjusting NUMFEATURES to 3 and 4 produced highest scores of 0.484, so I deduced that those two features to be the simplest and most effecetive.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\npd.set_option('max_colwidth', 10000)\nNUMFEATURES = 2\n\ncols = X_train.columns\n\nscore_df_columns = ['features', 'score', 'weights']\nscore_df = pd.DataFrame(columns = score_df_columns)\n\nfor combo in combinations(cols, NUMFEATURES):\n  combo = list(combo)\n  \n  # Logistic Regression\n  LR = LogisticRegression(max_iter = 20000000000)\n  LR.fit(X_train[combo], y_train)  \n  LRscore = cross_val_score(LR, X_train[combo], y_train, cv = 5).mean()\n  score_df.loc[len(score_df.index)] = [combo, LRscore, LR.coef_]  \n\nscore_df = score_df.sort_values(by='score', ascending=False).head(10)\nscore_df\n\n\n\n\n\n\n\n\nfeatures\nscore\nweights\n\n\n\n\n21\n[person_home_ownership, loan_percent_income]\n0.848736\n[[-1.0098629403833108, 8.271824027284113]]\n\n\n39\n[loan_int_rate, loan_percent_income]\n0.823416\n[[0.2896471726415302, 8.417607957665085]]\n\n\n32\n[loan_intent, loan_percent_income]\n0.819269\n[[-0.1108135938796114, 8.2548348300199]]\n\n\n27\n[person_emp_length, loan_percent_income]\n0.819138\n[[-0.04604467360459407, 8.17718472200731]]\n\n\n6\n[person_age, loan_percent_income]\n0.815471\n[[-0.0058903490094554785, 8.195442809295034]]\n\n\n43\n[loan_percent_income, cb_person_cred_hist_length]\n0.815340\n[[8.201924752324944, -0.007641209561773739]]\n\n\n20\n[person_home_ownership, loan_int_rate]\n0.814292\n[[-0.9816330580330728, 0.2767376475314839]]\n\n\n42\n[loan_percent_income, cb_person_default_on_file]\n0.813769\n[[8.373163550192576, 1.0910615242300115]]\n\n\n12\n[person_income, loan_amnt]\n0.808006\n[[-4.057354027170065e-05, 0.0001065591015240086]]\n\n\n31\n[loan_intent, loan_int_rate]\n0.803990\n[[-0.10705093453352543, 0.27916038916611374]]\n\n\n\n\n\n\n\nFrom the LogisticRegression.coef_ features, I was able to find the weights to use in my score function.\n\nweights = score_df.iloc[0][\"weights\"][0]\nweights\n\narray([-1.00986294,  8.27182403])\n\n\n\n\nFind a Threshold\nExtract the two desired features for the X_traindataset, and set the y_train dataset to be the predictor variable – loan_status\n\nX_train = df_train[['person_home_ownership', 'loan_percent_income']]\ny_train = df_train['loan_status']\n\nBefore we continue with making our model, let’s quickly check if it makes sense that these two variables are effective in modeling loan status. It turns out that yes, we see clear differences in loan_percent_income in the different person_home_ownership groups for those who did and didn’t default on their loans.\n\nsns.set_palette(\"dark\")\nplot1 = sns.barplot(df_for_viz, \n                        x = \"person_home_ownership\", \n                        y = \"loan_percent_income\", \n                        hue = \"loan_status\")\n\nplt.legend(title = \"Default on File\")\n\nAttributeError: 'numpy.int64' object has no attribute 'startswith'\n\n\n\n\n\n\n\n\n\nThe linear score for each entry of X_train is just X_train @ weights, where @ represents matrix multiplication.\n\ndef linear_score(X, w):\n    return X@w\n\ns = linear_score(X_train, weights)\n\nLet’s take a look at the scores we produced. All scores are between -3 and 5, with most being around 0. The distribution of scores roughly takes the form of a bell curve.\n\nhist = plt.hist(s)\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nWhat should the threshold be to maximize profit? To answer this question, I chose to copy df_train into df_profit. This way, I could add a column y_pred, which is False if the person’s score is less than the threshold and True if it is greater than or equal to the threshold. With a for loop, different values of threshold t can be considered. Another column, outcome, labels entries as true negatives, false negatives, false positives, and true positives. That way, each person’s profit contribution to the bank can be stored in a final column: profit. True positives and false positives don’t contribute any profit (neither positive nor negative) because they won’t be offered a loan in the first place – these people are expected to default. To find the average profit per loan, divide by the combined number of false negatives and true negatives\n\ndf_profit = df_train\n\nT = np.linspace(s.min()-0.1, s.max()+0.1, 101)\nprofits = []\n\nfor t in T:\n\n    y_pred = (s &gt;= t)\n\n    df_profit['y_pred'] = y_pred\n    df_profit['outcome'] = \"empty\"\n    df_profit['profit'] = 0.0\n\n    # label entries as true/false negative/positive\n    df_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 0), 'outcome'] = 'TN' # gain money\n    df_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 1), 'outcome'] = 'FN' # lose money\n    df_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 0), 'outcome'] = 'FP' # won't give loan\n    df_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 1), 'outcome'] = 'TP' # won't give loan\n    \n    # true positive -- bank makes money!\n    df_profit.loc[df_profit['outcome'] == 'TN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**10 - (df_profit['loan_amnt'])\n\n    # False negative -- bank loses money\n    df_profit.loc[df_profit['outcome'] == 'FN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**3 - (1.7*df_profit['loan_amnt'])\n\n    # total numbers of false negatives and true negatives, to calculate avg profit/ accepted loan (pred = False)\n    numFN = df_profit.loc[df_profit['outcome'] == 'FN'].size\n    numTN = df_profit.loc[df_profit['outcome'] == 'TN'].size\n    \n    if (numFN + numTN != 0):\n        totalprof = sum(df_profit['profit']) / (numFN + numTN)\n        profits.append(totalprof)\n    else:\n        profits.append(0)\n\nPlotting the average profit per loan results for each threshold value shows a maximum when threshold = -2. The expected profit per loan is around $110 at this value.\n\nimport matplotlib.ticker as ticker\n\nplt.plot(T, profits)\nplt.gca().set(ylim = (0, 120), xlim = (-4, 4))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\nEvalute your Model from the Bank’s Perspective\nLet’s run our model on the test set, with a threshold value of -2.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test =df_test.dropna()\n\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OTHER', 0)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('RENT', 1)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('MORTGAGE', 2)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OWN', 3)\n\n\nX_test = df_test[['person_home_ownership', 'loan_percent_income']]\n\n\n\n\n\n\n\n\nperson_home_ownership\nloan_percent_income\n\n\n\n\n0\n1\n0.02\n\n\n1\n2\n0.29\n\n\n2\n1\n0.06\n\n\n3\n2\n0.15\n\n\n4\n1\n0.08\n\n\n...\n...\n...\n\n\n6511\n2\n0.23\n\n\n6513\n1\n0.29\n\n\n6514\n3\n0.22\n\n\n6515\n2\n0.09\n\n\n6516\n1\n0.16\n\n\n\n\n5731 rows × 2 columns\n\n\n\n\nt = -2\ns_test = linear_score(X_test, weights)\ny_pred_test = (s_test &gt;= t)\n\n\ndf_profit = df_test.copy()\n\ndf_profit['y_pred'] = y_pred_test\ndf_profit['outcome'] = \"empty\"\ndf_profit['profit'] = 0.0\n\ndf_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 0), 'outcome'] = 'TN' # gain money\ndf_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 1), 'outcome'] = 'FN' # lose money\ndf_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 0), 'outcome'] = 'FP' # won't give loan\ndf_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 1), 'outcome'] = 'TP' # won't give loan\n    \n# true positive -- bank makes money!\ndf_profit.loc[df_profit['outcome'] == 'TN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**10 - (df_profit['loan_amnt'])\n\n# False negative -- bank loses money\ndf_profit.loc[df_profit['outcome'] == 'FN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**3 - (1.7*df_profit['loan_amnt'])\n\nnumFN = df_profit.loc[df_profit['outcome'] == 'FN'].size\nnumTN = df_profit.loc[df_profit['outcome'] == 'TN'].size\n    \ntotalprofit = sum(df_profit['profit']) / (numFN + numTN)\n\nprint(totalprofit)\n\n139.52875179012145\n\n\nThe expected profit per borrower is ~$140. This is $30 larger, or 27% larger, than the average profit per loan on the test set.\n\n\nEvaluate model from Borrower’s Perspective\nOur model produces great profits for the bank. But how does it function from the borrower’s perspective?\nThe following table shows one thing very clearly: this model does NOT grant loans to many individuals. In order to maximize the profit per loan, the model only predicts people to not default if they have a score &lt;-2. Considering the histogram of scores from earlier, we know that a very small subset of people in our training set had scores &lt;-2. In our test set, that’s only 174 people out of the total 5731.\n\ndf_profit.groupby('y_pred').size()\n\ny_pred\nFalse     174\nTrue     5557\ndtype: int64\n\n\nLet’s consider: is it more difficult for people in certain age groups to access credit? The following histogram indicates that no, our model accepts people from different age groups at equal rates.\n\n\nfig, axs = plt.subplots(1,2)\nplt.title(\"Distribution of Loan Intent by Age\")\nplt.subplots_adjust(hspace = 0.9)\n\n# is it more difficult for people in certain age groups to access credit?\nsns.histplot(data = df_profit, x = \"person_age\", hue = \"y_pred\", bins = 25, edgecolor = 'grey', ax = axs[0]).set(xlabel = \"person age\", ylabel = \"count\")\nsns.histplot(data = df_profit[df_profit['y_pred'] == False], x = \"person_age\", hue = \"y_pred\", bins = 25, edgecolor = 'grey', ax=axs[1])\n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nHowever, considering the following table, there are unfair trends. People 50+ are predicted to default 100% of the time and are consequently offered no loans by our model. It is easiest for people aged 40-50 to receive a loan; although they are expected to default 95.45% of the time, this is the lowest percentage of any age group.\n\nbins = [20, 30, 40, 50, 60, 70] \ndf_profit['Age Group'] = pd.cut(df_profit['person_age'], bins=bins, labels=['20-30', '30-40', '40-50', '50-60', '60-70'])\nsummary_stats = df_profit.groupby('Age Group').aggregate('y_pred').mean()\nsummary_stats\n\n/var/folders/nd/3yjvm85j3rq1vhh53yn6cy0r0000gn/T/ipykernel_63697/368891113.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  summary_stats = df_profit.groupby('Age Group').aggregate('y_pred').mean()\n\n\nAge Group\n20-30    0.969766\n30-40    0.970776\n40-50    0.954545\n50-60    1.000000\n60-70    1.000000\nName: y_pred, dtype: float64\n\n\nNow to consider: is it more difficult for people to get loans in order to pay for medical expenses? Medical loans are the 2nd most challenging to secure with our model, after Debt Consolidation.\n\n# predictions for default in groups\ndf_profit.groupby('loan_intent').aggregate('y_pred').mean()\n\nloan_intent\nDEBTCONSOLIDATION    0.997788\nEDUCATION            0.963435\nHOMEIMPROVEMENT      0.965909\nMEDICAL              0.971109\nPERSONAL             0.970942\nVENTURE              0.950207\nName: y_pred, dtype: float64\n\n\nWhen compared to the actual rate of default, people in the test set defaulted on debt consolidation loans 28.76% of the time, and defaulted on medical loans 28.43% of the time.\n\n# actual rate of default:\ndf_profit.groupby('loan_intent').aggregate('loan_status').mean()\n\nloan_intent\nDEBTCONSOLIDATION    0.287611\nEDUCATION            0.167517\nHOMEIMPROVEMENT      0.250000\nMEDICAL              0.284250\nPERSONAL             0.220441\nVENTURE              0.146266\nName: loan_status, dtype: float64\n\n\nFinally, I am curious how a person’s income level impacts the ease with which they can access credit under my decision system. To answer this question, I made the following table which shows the percentages of people in each income group who would be granted a loan by my algorithm. Unfortunately, only 2.38% of people making &lt;$50,000 would be offered a loan by my model, whereas people making $150,000 - 200,000 would be offered a loan 5.13% of the time. People making $200,000+ would be granted loans at a much higher rate: 9.19%.\n\n# percentages of people predicted not to default\n\n# Define the income groups\nbins = [0, 50000, 100000, 150000, 200000, np.inf]\nlabels = ['0-50,000', '50,000-100,000', '100,000-150,000', '150,000-200,000', '200,000+']\n\n# Create a new column for income groups\ndf_profit['income_group'] = pd.cut(df_profit['person_income'], bins=bins, labels=labels)\n\n# Group by income_group and y_pred and count the number of rows\ngrouped = df_profit.groupby(['income_group', 'y_pred']).size()\n\n# Calculate the percentage of rows where y_pred is False in each income group\npercentage = grouped.xs(False, level='y_pred') / grouped.groupby(level='income_group').sum() * 100\n\nprint(percentage.reset_index())\n\n      income_group         0\n0         0-50,000  2.378073\n1   50,000-100,000  3.041216\n2  100,000-150,000  4.570384\n3  150,000-200,000  5.128205\n4         200,000+  9.195402\n\n\n/var/folders/nd/3yjvm85j3rq1vhh53yn6cy0r0000gn/T/ipykernel_63697/537452620.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped = df_profit.groupby(['income_group', 'y_pred']).size()\n/var/folders/nd/3yjvm85j3rq1vhh53yn6cy0r0000gn/T/ipykernel_63697/537452620.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  percentage = grouped.xs(False, level='y_pred') / grouped.groupby(level='income_group').sum() * 100\n\n\n\n\nConclusion\nIn this exercise, I blindly set out with one goal: maximizing profit. I intentionally put my opinions aside to determine what the outcome would be if I truly just tried to make a model that makes the most money for a bank. Consequently, I used the “best” features for modeling loan status and set a very low threshold value that offered loans to very very few people – only ones rated very low chance of default. These people, unsurprisingly, tended to have high income, be in their 30s and 40s, and be requesting loans for venture use.\nOk, so I’ve made this brutal algorithm, now I can finally bring my brain back into the equation and think about what I’ve done. The people the mythical Liz Bank is offering loans to… don’t NEED the loans as badly as the others. They’re already rich! This hardly feels fair – people who have more money to begin with can get loans for more money, while others can’t?\nNow I need to describe how I like to consider fairness. I like thinking about it in terms of equality vs. equity. In an equal world, everyone is treated the same no matter their circumstances. In an equitable world, people with less advantages are given a “boost” so that people have near equal opportunities. I think that a fair world is an equitable world, but not necessarily an equal world.\n\n\n\ncitation:https://mostly.ai/blog/we-want-fair-ai-algorithms-but-how-to-define-fairness\n\n\nThis was a valable exercise. My mom works on a loan committee at a small local bank, and now I am curious how she and her colleagues determine who to grant loans to in our community. I am excited to ask her more about this, any models they use in their decisions, and maybe offer her some new perspective on the issue. This exercise reminded me of the importance of staying connected to the problem I’m solving with ML: things get dicey when there’s only one goal in mind (e.g: maximize profit!)"
  }
]