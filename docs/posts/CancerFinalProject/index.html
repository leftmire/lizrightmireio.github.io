<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Liz Rightmire">
<meta name="dcterms.date" content="2024-05-16">
<meta name="description" content="My final project on skin cancer image classification">

<title>Liz’s CSCI 0451 Blog - Final Project Blog Post</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<style>

      .quarto-title-block .quarto-title-banner h1,
      .quarto-title-block .quarto-title-banner h2,
      .quarto-title-block .quarto-title-banner h3,
      .quarto-title-block .quarto-title-banner h4,
      .quarto-title-block .quarto-title-banner h5,
      .quarto-title-block .quarto-title-banner h6
      {
        color: white;
      }

      .quarto-title-block .quarto-title-banner {
        color: white;
background-image: url(../../img/landscape.png);
background-size: cover;
      }
</style>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Liz’s CSCI 0451 Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Final Project Blog Post</h1>
                  <div>
        <div class="description">
          My final project on skin cancer image classification
        </div>
      </div>
                </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Liz Rightmire </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 16, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="project-blog-post" class="level1">
<h1>Project Blog Post</h1>
<section id="on-skin-cancer-identification-using-cnn-and-logistic-regresison-models" class="level3">
<h3 class="anchored" data-anchor-id="on-skin-cancer-identification-using-cnn-and-logistic-regresison-models">On skin cancer identification using CNN and Logistic Regresison models</h3>
</section>
<section id="abstract" class="level3">
<h3 class="anchored" data-anchor-id="abstract">Abstract</h3>
<p>Skin cancer is the most common form of cancer. According to the Victoria department of health, over 95% of skin cancers that are dected early can be successfully treated. This means that early detection is a crucial step in the treatment process of skin cancer. In our analyses, we aim to identify different types of skin conditions with image classification techniques. Using convolutional neural networks (CNN) and logistic regression (LR) models, we obtain a 75% accuracy rate for classification, where melanocytic nevi (NV) and melanoma (MEL) are the most successfully classified forms of skin condition.</p>
<p>Link to GitHub Repository: <a href="https://github.com/ShadowedSpace/cancer">link</a></p>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>In 1994, Binder et al.&nbsp;trained a neural network that successfully differentiated between melanomas, the leading cause of skin cancer deaths, and melanocytic nevus, which are generally harmless skin lesions commonly known as moles or birth marks. In 2018, a challenge hosted by the International Skin Imaging Collaboration (ISIC) lay out the task of detection and classification of skin lesions and diseases. 900 parties registered to download the data for the challenge and hundreds of groups submitted novel evaluation techniques to automate the process of diagnosing skin lesions and diseases. A study conducted on the ISIC challenge revealved that the best classification models still failed to properly classify on average over 10% of dermoscopic images and had varying abilities to generalize (Coedlla et al., 2019). As it turns out, Binder et al.’s study (1994) trained their neural network on 200 images. The dataset used by the ISIC challenge is the world’s largest public repository of dermoscopic images of skin and contains 10015 dermoscopic images. First released in 2018 by Philipp Tschandl et al., the HAM10000 (Humans Against Machines) is a novel dataset with the aim of improving the process of automated skin lesion diagnosis, as most existing demoscopic image datasets are either small in size or lacking diversity.</p>
<p>According to a recent study by Tandon et al.&nbsp;(2024), the most successful deep learning model to implement for the automation of cancer diagnoses is the convolutional neural network (CNN). CNNs are a type of neural network that are particularly effective for identifying patterns in images and audio. CNNs are best used when there are large amounts of data to train the model on (MathWorks). Before the release of the HAM10000 dataset, the diversity and size of dermatoscopic images was limited, which also limited the progression of automated skin lesion detection (Tschandl et al.&nbsp;2024). Binder et al.(1994) creates a model from a much smaller sample size and is limited in the diversity of skin lesions it can detect. As revealed by the 2018 ISIC challenge, automated healthcare diagnoses are still limited in accuracy and generalizability.</p>
<p>In our project, we apply our knowledge of Convolutional Neural Networks (CNN) and other machine learning techniques to the HAM10000 dataset with the goal of classifying types of skin lesions. Accurate and early skin lesion diagnosis is crucial to a successful recovery when it comes to skin cancer. Melanoma, a type of skin cancer, is quick to spread to other organs, and is the leading cause of deaths due to skin cancer. But when detected early enough, before the melanoma has a chance to spread, melanoma has a five-year survival rate of 98%. Precancerous or skin cancers spots can be spotted by the naked eye. According to the Fred Hutch Center, the ABCDE Guice is a way to visually detect abnormalities in skin lesions:</p>
<p><strong>A</strong> for asymmetry- early melanomas are asymmetrical while moles are symmetrical</p>
<p><strong>B</strong> for border- early melanomas tend to have uneven borders</p>
<p><strong>C</strong> for color- early melanomas are often a vareity of shades of brown while common moles tend to be one shade</p>
<p><strong>D</strong> for diameter- the diameter of melanomas are typically larger than that of a mole</p>
<p><strong>E</strong> for evolution- changes in a skin lesion may indicate skin cancer</p>
<p>As seen in Figure 1, the majority of skin lesions in the HAM10000 dataset are melanocytic nevi (NV), and the second most common skin lesions are melanomas (MEL).</p>
<p>We use CNN and logistic regression models to classify types of skin lesions identified in the HAM10000 dataset. Using the logistic regression models on non-image variables provided in the dataset, we achieve a testing accuracy of 70%. Including images when training the model does not significantly increase the accuracy rate. When using the CNN method, we apply class weights as the dataset is highly imbalance towards NV diagnoses. This results in a model with 75% testing accuracy.</p>
<p>We also separate skin lesion types into ‘malignant’ and ‘benign’ categories. When training to classify more dangerous skin lesions from harmless skin lesions, the weighted logistic regression model is able to correctly identify 57% of demoscopic images showing malignant skin lesions. Compared to the CNN model of simply classifying skin lesions, where malignant skin lesions are correctly identified 3% to 16% of the time, grouping the malignant skin lesions for classification results in a much improved model for correctly indentifying malignant skin lesions.</p>
<p>While the imbalanced dataset made the methods we used more difficult to train to achieve higher accuracy than baseline, weighing these models achieves an accuracy of classification of 75%, which is about 10 percentage points greater than the baseline accuracy. Additionally, our findings from malignant skin lesion identification reveal that combining malignant skin lesion categories for classification significantly improves the model performace.</p>
</section>
<section id="values-statement" class="level3">
<h3 class="anchored" data-anchor-id="values-statement">Values Statement</h3>
<p>We created this model as a way to explore how technologies like convolutional neural networks could be used on a real-world problem. In the state it is in now, we do not recommend using our model as a true diagnosis tool. However, if we were to continue to improve this model to higher accuracy, there is a world where community members could upload photos of skin lesions and receive a percentage risk score that their skin lesion might benefit from analysis from a professional. That being said, an image would need to be high-quality and under professional lighting to match the image style of the images in HAM10000.</p>
<p>One of the greatest challenges in the healthcare industry is access. Geographical “deserts” exist all across the country where population healthcare needs are unmet partially or totally due to lack of adequate access (Brinzac et al). TeleHealth and WebHealth solutions are aiding these communities; a skin lesion classification algorithm could be used in such services.</p>
<p>The HAM10000 dataset was created to address the issue of small size and lack of diversity in publically available skin cancer datasets. That being said, the images were collected from the Austrian and Australian population, consisting of predominantly white individuals. We have not tested how our algorithm’s results when applied to skin lesions on darker skin tones, but it would likely underperform.</p>
<p>Speak with just about anyone and it’s likely that they or somebody they love has been touched by cancer. This is certainly true for the four of us, especially Zoe who received a cancer diagnosis last spring. We combined Liz’s interest in applying machine learning to improve health outcomes with Zoe’s specific interest in cancer, and settled on the HAM10000 dataset because of its wide use.</p>
<p>It is our hope that the world would be a more joyful place with the implementation of this algorithm. Beacuse of the crucial role of early recognition in skin cancer diagnosis, we hope that this model could allow people to more readily determine if they should get a skin lesion evaluated by a medical professional.</p>
</section>
<section id="materials-and-methods" class="level3">
<h3 class="anchored" data-anchor-id="materials-and-methods">Materials and Methods</h3>
<section id="data" class="level5">
<h5 class="anchored" data-anchor-id="data">Data</h5>
<p>For this project, we used the publicly available HAM10000 dataset, found on the Harvard Dataverse (Tschandl, 2023). It includes 10015 dermatoscopic images of skin lesions which were collected over a period of 20 years at two different sites, one in Austria and the other in Australia (Tschandl et al.&nbsp;2018).</p>
<p>Along with the image data, we used the provided metadata file that contained information such as lesion_id, image_id, dx, dx_type, age, sex, localization, dataset. Some lesions were photographed multiple times, and dx refers to the diagnosis. One of the first challenges was converting the images into 2D numpy arrays and adding them into the data frame with the dx information (the target we were attempting to predict).</p>
<p>The limitations of this data were immediately obvious. This includes bias in the type of skin lesions represented in the data, in addition to bias of the demographics and skin types of patients with lesions represented in the data. As mentioned above, the data was collected in Australia and Austria, meaning that darker skin tones were not represented. This is particularly important to consider in a data set being used for machine learning, because if the machine doesn’t learn on a fair data set, there is likely to be a trickle down effect into the health care system, further systematizing racial injustices in patient treatment.</p>
</section>
<section id="figure-1-frequency-of-skin-lesion-categories" class="level4">
<h4 class="anchored" data-anchor-id="figure-1-frequency-of-skin-lesion-categories">Figure 1: Frequency of Skin Lesion Categories</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/SkinCancer_TypeFrequency.png" title="Skin Lesion Categories" class="img-fluid figure-img"></p>
<figcaption>alt text</figcaption>
</figure>
</div>
<p>This figure shows the frequency of diagnoses present in the data set. Clearly, the nv dx dominates the space, bringing up questions about how well a model trained on this data will classify any non-nv diagnoses. This issue will be further discussed below in the approach section.</p>
<section id="approach" class="level5">
<h5 class="anchored" data-anchor-id="approach">Approach</h5>
<p>The features of our data used to make predictions were the images, converted into 32x32x3 numpy arrays. The goal was to predict the column of the metadata table labeled dx, the diagnosis corresponding to each lesion.</p>
<p>We subset our data into two sets, 80% for the training set and 20% for the test set. This train-test split provided a large enough sample to properly train the model, while also leaving enough data unseen to evaluate whether overfitting occurred.</p>
<p>To start out, we used the sci kit learn LogisticRegression model. This was mostly a preliminary effort to eliminate any bugs and see how we were doing in comparison to the base accuracy. After this, we moved on to our main model, which was a convolutional neural network (CNN). This decision was primarily based off previously published papers showing that this method was the most successful. We attempted several variations, each with and without data augmentation. First, we tried a minimally connected linear model. Then we added additional ReLU and Conv2D layers to see if the additional layers would improve the accuracy. We also attempted a model with added dropout layers, and a dropout probability of 25%. We started to be suspicious of the results when a model recommended by a published paper was not yielding good accuracy. After investigating this issue, we realized that because of the high prevalence of the dx nv in our dataset, the model was always choosing this as the correct answer. To fix this, we implemented class weights and transfer learning. Transfer learning proved more successful, yielding our best accuracy of around 75%.</p>
<p>Originally, we trained our models on 10 or 20 epochs. Once we isolated the best models, we spent the time to do a 100 epoch training period. All training was performed on our personal laptops.</p>
<p>Our models were evaluated on accuracy of the validation set after extensive training. Confusion matrices were used extensively to investigae in which categories the misclassifications were occurring. As mentioned above, the training set contained 80% of the data (~8000 images) while the validation set was comprised of 20% (~2000 images).</p>
</section>
</section>
</section>
<section id="results" class="level3">
<h3 class="anchored" data-anchor-id="results">Results</h3>
<p>Our best models were the product of data augmentation, class weights, and transfer learning. First, we trained an ImageNet model, as designed by Stanford reserachers in 2009. Training for 100 epochs took about 240 minutes but produced an accuracy of about 70% on a validation set. The accuracy steadily increased over the epochs:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ImageNetTraining.jpg" class="img-fluid figure-img"></p>
<figcaption>TrainingGraph</figcaption>
</figure>
</div>
<p>Take a look at the confusion matrix:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ImageNet_Confusion.jpg" class="img-fluid figure-img"></p>
<figcaption>ConfusionMatrix</figcaption>
</figure>
</div>
<p>Our model is best at predicting nv, or melanocytic nevi. Unfortunately, this is a benign skin lesion. It is the majority class in our model, explaining why it is predicted most often. That being said, we see better accuracy with melanoma and benign keratosis than previous models.</p>
<p>With the intention of increasing classification accuracy for melanoma, the most dangerous skin cancer, we utilized mobilenet_v2, Google’s transfer learning model from 2007. By adjusting the class weights to target the minority classes more aggressively, we were able to produce the following results on validation data.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/mobilenet_confusion.jpg" class="img-fluid figure-img"></p>
<figcaption>ConfusionMatrix</figcaption>
</figure>
</div>
<p>We see increased accuracy for melanoma, at the expense of melanocytic nevi.</p>
</section>
<section id="concluding-discussion" class="level3">
<h3 class="anchored" data-anchor-id="concluding-discussion">Concluding Discussion</h3>
<p><em>Your conclusion is the right time to assess:</em></p>
<p><em>In what ways did our project work?</em> <em>Did we meet the goals that we set at the beginning of the project?</em> <em>How do our results compare to the results of others who have also studied similar problems?</em> <em>If we had more time, data, or computational resources, what might we do differently in order to improve further?</em></p>
<p>Our project was less successful than we had initially hoped. Several of our members have been impacted by cancer outside of this project, so we were particularly motivated to create a model that could be used to help people. While our intentions and motivations were good, our results were not as successful as we had hoped. We were able to achieve a 70% accuracy rate, which is better than the baseline accuracy of 65%. However, this is still not a high enough accuracy rate to be used as an accurate diagnostic tool. We had hoped that our model would provide an accessible alternative to professional diagnosis, but it was instead only the first step in a long process.</p>
<p>While we most certainly did not cure cancer, we met many of our exploration and implementation goals. Our model has some predictive power, and we were able to explore the HAM10000 dataset in depth. We were able to implement a convolutional neural network, which is a powerful tool for image classification and we were able to explore the limitations of our model and the dataset. We were also able to implement class weights and transfer learning to improve our model’s accuracy. These fully fulfilled our goals of utilizing machine learning techniques to classify skin lesions.</p>
<p>Our results were within the range of similarly experienced projects using the HAM10000 dataset. However, one group of researchers was able to achieve an accuracy of 84.3% with an EfficientNet model, which is a more advanced model than the ones we used (Tajerian et al., 2023). Another group achieved an accuracy of 96.15 using Google’s ViT patch-32 model (Himel et al., 2024), which is also a more advanced model than the ones we used. Our results were not as successful as these groups, but they were within the range of other groups that used simpler models.</p>
<p>If we’d had more time and resources to continue this project, we likely would have explored some of the more advanced models mentioned above, as well as attempted to replicate some of their results. We were also limited by the size of the dataset, which, while relatively small for a machine learning project, is rather large to be processed on a personal laptop. We would have liked to further explore some of the ethical implications of our model, such as the potential for racial bias in the data and the potential for our model to be used as a diagnostic tool in underserved communities.</p>
</section>
<section id="group-contribution-statement" class="level3">
<h3 class="anchored" data-anchor-id="group-contribution-statement">Group Contribution Statement</h3>
<p>Our group was formed because the topics proposed by Liz and Zoe had similarities regarding identification in the health space. Once we decided to look into cancer diagnoses, Zoe presented several dataset options to the group. We decided to focus on skin cancer identification and looked into datasets as a group, ultimately landing on the HAM10000 dataset. In doing the analyses, we decided to split the workload so that Liz and Zoe work primarily on the building the convolutional neural network, while Julia and Breanna worked on improving the logistic regression models. For the final blog post, Breanna wrote the abstract, introduction and group contribution statement. Julia summarized the concluding thoughts from the project. Liz wrote the values statement and worked on the results section jointly with Zoe. Zoe worked with Liz on the results and wrote the data/methods section.</p>
</section>
<section id="personal-reflection" class="level3">
<h3 class="anchored" data-anchor-id="personal-reflection">Personal Reflection</h3>
<p><em>At the very end of your blog post, in a few paragraphs, respond to the following questions:</em></p>
<p><em>What did you learn from the process of researching, implementing, and communicating about your project?</em> <em>How do you feel about what you achieved? Did meet your initial goals? Did you exceed them or fall short? In what ways?</em> <em>In what ways will you carry the experience of working on this project into your next courses, career stages, or personal life?</em></p>
</section>
<section id="sources" class="level3">
<h3 class="anchored" data-anchor-id="sources">Sources</h3>
<p>Binder, M., A. Steiner, M. Schwarz, S. Knollmayer, K. Wolff, and H. Pehamberger. 1994. “Application of an Artificial Neural Network in Epiluminescence Microscopy Pattern Analysis of Pigmented Skin Lesions: A Pilot Study.” British Journal of Dermatology 130 (4): 460–65. https://doi.org/10.1111/j.1365-2133.1994.tb03378.x.</p>
<p>Codella, Noel, Veronica Rotemberg, Philipp Tschandl, M. Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, et al.&nbsp;2019. “Skin Lesion Analysis Toward Melanoma Detection 2018: A Challenge Hosted by the International Skin Imaging Collaboration (ISIC).” arXiv. https://doi.org/10.48550/arXiv.1902.03368.</p>
<p>Services, Department of Health &amp; Human. n.d. “Melanoma.” Department of Health &amp; Human Services. Accessed May 13, 2024. http://www.betterhealth.vic.gov.au/health/conditionsandtreatments/melanoma.</p>
<p>Himel, Galib Muhammad Shahriar et al.&nbsp;“Skin Cancer Segmentation and Classification Using Vision Transformer for Automatic Analysis in Dermatoscopy-Based Noninvasive Digital System.” International journal of biomedical imaging vol.&nbsp;2024 3022192. 3 Feb.&nbsp;2024, doi:10.1155/2024/3022192. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10858797/.</p>
<p>“Skin Cancer Early Detection.” n.d. Fred Hutch. Accessed May 13, 2024. https://www.fredhutch.org/en/patient-care/prevention/skin-cancer-early-detection.html.</p>
<p>Tajerian, Amin et al.&nbsp;“Design and validation of a new machine-learning-based diagnostic tool for the differentiation of dermatoscopic skin cancer images.” PloS one vol.&nbsp;18,4 e0284437. 14 Apr.&nbsp;2023, doi:10.1371/journal.pone.0284437. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10104315/.</p>
<p>Tandon, Ritu, Shweta Agrawal, Narendra Pal Singh Rathore, Abhinava K. Mishra, and Sanjiv Kumar Jain. 2024. “A Systematic Review on Deep Learning-Based Automated Cancer Diagnosis Models.” Journal of Cellular and Molecular Medicine 28 (6): e18144. https://doi.org/10.1111/jcmm.18144.</p>
<p>Tschandl, Philipp. 2023. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Harvard Dataverse. https://doi.org/10.7910/DVN/DBW86T.</p>
<p>Tschandl, Philipp, Cliff Rosendahl, and Harald Kittler. 2018. “The HAM10000 Dataset, a Large Collection of Multi-Source Dermatoscopic Images of Common Pigmented Skin Lesions.” Scientific Data 5 (1): 180161. https://doi.org/10.1038/sdata.2018.161.</p>
<p>“What Is a Convolutional Neural Network? | 3 Things You Need to Know.” n.d. Accessed May 13, 2024. https://www.mathworks.com/discovery/convolutional-neural-network.html.</p>
<p>Brinzac, Monica, Kuhlmann, Ellen, Dussault, Gilles. “Defining medical deserts – an international consensus-building exercise.” PubMed, National Center for Biotechnology Information. https://pubmed.ncbi.nlm.nih.gov/37421651/#:~:text=Results%3A%20The%20agreed%20definition%20highlight.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>