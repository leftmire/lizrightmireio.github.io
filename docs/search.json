[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nMy name is Liz Rightmire\nI’m a computer science student at Middlebury College\nThis blog holds the work I’ve done in CSCI0451: Machine Learning"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Liz’s CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\nImplementing and Testing Logistic Regression\n\n\n\n\n\nApr 10, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nMid-Course Reflection\n\n\n\n\n\nWe reflect on our learning, engagement, and achievement in the first part of the semester. \n\n\n\n\n\nApr 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nImplementing and Testing the Perceptron Algorithm\n\n\n\n\n\nMar 27, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nAn essay considering and critiquing methods used to ensure fair algorithms\n\n\n\n\n\nMar 14, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nWomen in Data Science Conference\n\n\n\n\n\nResearching gender disparities in data science, and attending Women in Data Science Conference\n\n\n\n\n\nMar 10, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nWhose Costs?\n\n\n\n\n\nDesigning and Evaluating a Profit-Maximizing Loan Default Predictor Model\n\n\n\n\n\nMar 1, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nSelecting features and model for accurate penguin species classifier\n\n\n\n\n\nFeb 20, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nGoal Setting\n\n\n\n\n\nGoal setting for beginning of semester\n\n\n\n\n\nJan 10, 2023\n\n\nLiz Rightmire\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ClassifyingPalmerPenguins/index.html",
    "href": "posts/ClassifyingPalmerPenguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "CSCI 0451\n\nClassifying Palmer Penguins\n\n\n\ncite: https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\n\n\nThe Palmer Penguins dataset is a public dataset frequently used within the educational data science community. It contains Dr. Kristen Gorman and the Long Term Ecological Research Network’s observations of hundreds of Antartic penguins belonging to 3 species groups: Adelie, Gentoo, and Chinstrap.\nThis blog aims to accomplish three goals:\n\nConduct exploration of the Palmer Penguins dataset\nMethodically select 3 features and a model type that produces 100% penguin species classification accuracy on test data\nEvaluate chosen model by analyzing decision regions and a confusion matrix\n\n\nRandom Exploration\n\nimport pandas as pd\n\n# import dataset\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nThe training data contains 18 observations about 275 penguin subjects. Let’s clean this data by dropping irrelevent columns and one-hot encoding the qualitative observations.\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize label encoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\n\ndef prepare_data(df):\n  '''Prepare data for model development'''\n\n  # drop extraneous columns\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n \n  # drop rows with empty \"Sex\" column\n  df = df[df[\"Sex\"] != \".\"]\n  \n  # drop rows with NA values\n  df = df.dropna()\n\n  # label encode Species column\n  y = le.transform(df[\"Species\"])\n\n  # remove Species column for training set\n  df = df.drop([\"Species\"], axis = 1)\n\n  # convert categorical variables into indicator variables\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\nVisualizations\n\nimport seaborn as sns\nsns.set_palette(\"husl\", 3)\n\n# visualization 1: flipper length frequency by species\nsns.histplot(data = train, x = \"Flipper Length (mm)\", hue = \"Species\", bins = 25, edgecolor = 'grey')\n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis histogram shows the distribution of the penguins’ flipper lengths. The bars are colored based on penguin species, so the shape of the distributions allow the viewer to infer the mean, medians, and modes for each species. There is a trend in flipper length based on species: Gentoo penguins tend to have the largest flippers, and Adelie and Chinstrap penguins have smaller ones. This indicates that flipper length may be a helpful feature to use in classification, as it effectively identifies the Gentoo penguins from the rest of the species.\n\n# visualization 2: body mass vs. culmen length\nplot1 = sns.scatterplot(train, x = \"Body Mass (g)\", y = \"Culmen Length (mm)\", hue = \"Species\")\n\n\n\n\n\n\n\n\nAfter considering multiple combinations of qualitative features in scatterplots, body mass and culmen length proved to show clear groupings of penguin species. Therefore, these two features are likely to be effective in a classification model. That being said, overlap does exist, especially between Adelie and Gentoo penguins.\n\n# summary table\ntable = train.groupby(['Species', 'Island']).size()\ntable\n\nSpecies    Island   \nAdelie     Biscoe       33\n           Dream        45\n           Torgersen    42\nChinstrap  Dream        57\nGentoo     Biscoe       98\ndtype: int64\n\n\nThis summary table shows the numbers of penguins present on each island. Every penguin on Torgersen island is an Adelie penguin, and both Gentoo and Chinstrap penguins can only be found on one island. These clear trends cause me to consider island location as a potential feature in a classification model.\n\n\n\ncite: https://en.ac-illust.com/clip-art/22518802/illustration-of-a-cute-penguins-playing-a-computer\n\n\n\n\nChoosing Features\nBecause there were only 5 qualitative and 6 quantitative feature in the cleaned dataset, I chose to perform an exhaustive search to determine the most effective features and model for species classification. For each combination of 2 quantitative and 1 qualitative feature, I fit 4 models: Logistic Regression, Decision Tree, Random Forest, and SVM. Cross validation was performed to guard against overfitting as follows:\n\nLogistic Regression: recorded average accuracy of 5 rounds, each round with a random 20% of data used for testing\nDecision Tree and Random Forest: performed grid search of varrying max_depth values, recorded highest accuracy achieved\nSVM: performed grid search of varrying gamma values, recorded highest accuracy achieved\n\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\n# adjust default df column width so that all 3 features appear\npd.set_option('max_colwidth', 10000)\n\n# differentiate qualitative and quantitative columns\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)' ]\n\n# initialize accuracy score df\ncolumns = ['features', 'model', 'score']\nscore_df = pd.DataFrame(columns = columns)\n\nfor qual in all_qual_cols: \n  # choose a qualitative column\n  qual_cols = [col for col in X_train.columns if qual in col ]\n\n  # choose 2 quantitative columns\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n\n    ### Logistic Regression\n    LR = LogisticRegression(max_iter = 20000000000)\n    LR.fit(X_train[cols], y_train)  \n    LRscore = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n    score_df.loc[len(score_df.index)] = [cols, 'Logistic', LRscore]  \n\n    ### Decision Tree\n    DTC = DecisionTreeClassifier()\n    # perform grid search to find optimal max depth\n    param_grid = {'max_depth': [11,2,3,4,5,6,7,8,9,10, None]}\n    # grid search with cross-validation \n    grid_search = GridSearchCV(DTC, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCscore = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, 'Decision Tree', DTCscore]  \n\n    ### Random Forest\n    randomforest = RandomForestClassifier()\n    # cross validation for optimal max depth, with cross-validation\n    grid_search = GridSearchCV(randomforest, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    RF_score = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, 'Random Forest', RF_score]\n\n    ### SVM\n    param_grid = {'gamma': 10.0**np.arange(-5, 5)}\n    SVC_model = SVC()\n    # perform grid search for optimal gamma value, with cross-validation\n    grid_search = GridSearchCV(SVC_model, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    SVMscore = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, \"SVM\", SVMscore]\n\n# print out results df, sorted by accuracy \"score\"\nscore_df.sort_values(by='score', ascending=False).head(10)\n\n\n\n\n\n\n\n\nfeatures\nmodel\nscore\n\n\n\n\n60\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nLogistic\n0.988311\n\n\n62\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nRandom Forest\n0.988311\n\n\n66\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\nRandom Forest\n0.984465\n\n\n126\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Flipper Length (mm)]\nRandom Forest\n0.984389\n\n\n122\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\nRandom Forest\n0.984389\n\n\n120\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\nLogistic\n0.984389\n\n\n166\n[Island_Biscoe, Island_Dream, Island_Torgersen, Flipper Length (mm), Delta 13 C (o/oo)]\nRandom Forest\n0.984314\n\n\n138\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Delta 13 C (o/oo)]\nRandom Forest\n0.980543\n\n\n63\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nSVM\n0.980543\n\n\n134\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Delta 15 N (o/oo)]\nRandom Forest\n0.976621\n\n\n\n\n\n\n\nFor each model, the features, model type, and highest possible score was added to a dataframe. Sorting this dataframe by score revealed that Logistic Regression with Sex, Culmen Length and Culmen Depth as features produced the best classification.\n\n\n\ncitation: https://www.dreamstime.com/stock-illustration-d-penguin-teaches-math-render-numbers-image45736629\n\n\n\n\nEvaluate Chosen Model\nTo truly evaluate our model, we must evaluate how it performs on unseen testing data.\n\n# train with optimal features and model\nLR = LogisticRegression()\noptimal_features = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\nLR.fit(X_train[optimal_features], y_train)  \n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n#test\nX_test, y_test = prepare_data(test)\nLR.score(X_test[optimal_features], y_test)\n\n1.0\n\n\nA score of 1.0 indicates that 100% of the penguins in the testing dataset were correctly classified by our model. Yippie!\nStepping back a little, let’s consider: does this make sense? Should we be able to determine a penguin species based on its sex, culmen length, and culmen depth?\nYes, it seems logical that different species of penguins have different culmen dimensions. I am also not surprised that sex is an important qualitative feature to consider, as penguin size, and consequently beak dimensions, vary based on sex. For example, female Gentoo penguins may have similar culmen lengths to a male Adelie, so sex is required to determine species.\nTo be sure, let’s create a scatterplot of these 3 features and look at the decision regions produced by our logistic algorithm.\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the training data:\n\n# training data\nplot_regions(LR, X_train[optimal_features], y_train)\n\n\n\n\n\n\n\n\nFor the testing data:\n\n# testing data\nplot_regions(LR, X_test[optimal_features], y_test)\n\n\n\n\n\n\n\n\nYes, separating penguins by sex creates very clear clusters of points by species in the testing data, which transfers perfectly to the points in the testing data.\nAnother way to evaluate would be to look at a confusion matrix for this model\n\nfrom sklearn.metrics import confusion_matrix\n\n# guesses for each data\ny_test_pred = LR.predict(X_test[optimal_features]) \n\n# confusion matrix\ncm = confusion_matrix(y_test, y_test_pred)\n\n# print confusion matrix results\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {cm[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\nZero penguins were mis-classified. This makes sense because we got our logistic regression model had an accuracy of 1.0!\nThe plot below is another way to visualize the confusion matrix. For an accuracy of 1.0, we would expect zeros in every box execept for those on the diagonal, meaning that zero penguins were misclassified.\n\n# plot confusion matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ncm_df = pd.DataFrame((cm), index = ['Gentoo','Chinstrap ','Adelie'], columns = ['Gentoo','Chinstrap','Adelie'])\ndisplay_labels = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nConfusionMatrixDisplay(cm, display_labels = display_labels).plot(\n    include_values=True)\n\n\n\n\n\n\n\n\n\n\nDiscussion\nIn this blog post, I aimed to explore the popular Palmer Penguins dataset and build a classification model that could correctly determine the species of every penguin in unseen data. In accomplishing these goals, I created two new visualizations and summary table, developing skills using pandas functions and matplotlib. I also practiced selecting features for a model by testing feature combinations on 4 model types – logistic regression, decision tree, random forest, and SVM. I optimized each model using cross validation and grid search for optimal hyperparameters, and evaluated each model based on accuracy.\nOnce I had settled on the best features and best model type, I evaluated my model on unseen testing data and achieved the desired result – 100% classification accuracy! I created and interperated a confusion matrix, which corroborated my high accuracy result.\nThe skills I learned in this blog post will surely serve me well as I move into understanding and developing more complex models."
  },
  {
    "objectID": "posts/Goal-Setting/goal-setting.html",
    "href": "posts/Goal-Setting/goal-setting.html",
    "title": "Goal Setting",
    "section": "",
    "text": "Liz Rightmire\n\n\nThe knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs I progress through the computer science major, I have developed a specific interest in data science and machine learning – it’s what I hope to do postgrad. I’ve been looking forward to this class for a long time, and when looking at my schedule, it seems within the realm of possibility to make this class a priority. I have a bit of experience with with implementation and experimentation of ML through a data science internship last summer, but I am far from an expert :). Therefore, I hope to apply most of my focus to theory (I want to better understand HOW these algorithms work and leverage my mathematical experience!) and Social Responsibliity (with an aspired career in data science, I am concerned about my impact and I want to take advantage of my liberal arts education to understand the harm that can be done in this industry.)\n\n\n\n\n\nMost blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI hope to do most of the blog posts. I tend to work at a slower rate than the average student, and considering that they take 5-8 hours to complete, I want to leave room in my goal of 10hrs/week of ML for readings and warm-ups. I see myself learning a lot from the blog posts, so I want to do as many as possible without over-extending myself. I find it challenging to quantify a specific number of blog posts at this time. Instead, it feels more reasonable to say that knowing that we’ll have check-ins throughout the semester with the average numbers completed by the class, I hope my statistic will place me in an ambitous range.\nWhen I need to prioritize which blog posts to complete, I want to cover all categories but place special emphasis on Theory and Social Responsibility. I hope to propose and complete one additional blog post on Social Responsibility. Knowing myself, I don’t think it will be challenging to submit my work by the best-by date and I want to give myself flexibility to invest extra time into blog posts that particularly interest me!\nI believe that completing revisions is where I’ll learn the most, so I want to revise most (5?) of my blog posts to “no revisions suggested.”\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI aspire to do all of the readings – including some optional theory readings – as well as all of the warm-ups to the best of my ability. It doesn’t feel productive to place a maximum number of “passes” because I trust my effort and intentions in doing my best work; instead, I’ll try to attend student hours if I get stumped. I might need help from classmates sometimes, and I think that’s ok.\nBefore class, I aspire to preview the in-class coding we’ll be doing so that I prep myself well for understanding the content.\nI have already begun working with some classmates outside of class, so I hope to sometimes be the one to organize work times.\nSomething I want to work on is my presentation skills. I presented on an algorithm for my J-Term class, and I felt poorly about how it went. Each time I’m chosen to present the warm-up in ML, I want to treat it as practice speaking in front of others and engaging my peers in discussion. I’m not one to speak often in class, but I aspire to ask a few questions over the semester and maybe even answer some questions. More practical for me, perhaps, is to challenge myself to attend student hours to ask specific questions when I get stuck.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI look forward to working on a culmianting project with some classmates! Right now, I think I’ll be interested in finding a complex dataset and applying some of the techniques we’ve learned in class as well as trying some new techniques. At the end of the project, I want to feel as though I “pulled my weight” with technical contributions, but also practice my interpersonal teamwork skills and hold teammates accountable for work. The final presentation will be a great opportunity for me to apply what I’ve been practicing in leading warm-ups."
  },
  {
    "objectID": "posts/Goal-Setting/goal-setting.html#what-youll-learn",
    "href": "posts/Goal-Setting/goal-setting.html#what-youll-learn",
    "title": "Goal Setting",
    "section": "",
    "text": "The knowledge we’ll develop in CSCI 0451 can be broadly divided into four main areas:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nEvery student should grow toward each of these areas, but you can choose to specialize if you’d like! If there are one or two of these categories on which you’d especially like to focus, list them below. Feel free to include any details that you’d like – this can help me tailor the course content to your interests.\nAs I progress through the computer science major, I have developed a specific interest in data science and machine learning – it’s what I hope to do postgrad. I’ve been looking forward to this class for a long time, and when looking at my schedule, it seems within the realm of possibility to make this class a priority. I have a bit of experience with with implementation and experimentation of ML through a data science internship last summer, but I am far from an expert :). Therefore, I hope to apply most of my focus to theory (I want to better understand HOW these algorithms work and leverage my mathematical experience!) and Social Responsibliity (with an aspired career in data science, I am concerned about my impact and I want to take advantage of my liberal arts education to understand the harm that can be done in this industry.)"
  },
  {
    "objectID": "posts/Goal-Setting/goal-setting.html#what-youll-achieve",
    "href": "posts/Goal-Setting/goal-setting.html#what-youll-achieve",
    "title": "Goal Setting",
    "section": "",
    "text": "Most blog posts will require around 5-8 hours on average to complete, plus time for revisions in response to feedback. Blog posts will most frequently involve a mix of mathematical problem-solving, coding, experimentation, and written discussion. Some blog posts will ask you to critically discuss recent readings in an essay-like format.\nI hope to do most of the blog posts. I tend to work at a slower rate than the average student, and considering that they take 5-8 hours to complete, I want to leave room in my goal of 10hrs/week of ML for readings and warm-ups. I see myself learning a lot from the blog posts, so I want to do as many as possible without over-extending myself. I find it challenging to quantify a specific number of blog posts at this time. Instead, it feels more reasonable to say that knowing that we’ll have check-ins throughout the semester with the average numbers completed by the class, I hope my statistic will place me in an ambitous range.\nWhen I need to prioritize which blog posts to complete, I want to cover all categories but place special emphasis on Theory and Social Responsibility. I hope to propose and complete one additional blog post on Social Responsibility. Knowing myself, I don’t think it will be challenging to submit my work by the best-by date and I want to give myself flexibility to invest extra time into blog posts that particularly interest me!\nI believe that completing revisions is where I’ll learn the most, so I want to revise most (5?) of my blog posts to “no revisions suggested.”\n\n\n\nYou make a choice each day about how to show up for class: whether you’ll be prepared, whether you’ll engage with me and your peers in a constructive manner; and whether you’ll be active during lecture and discussions.\nAn especially important form of course presence is the daily warmup. We’ll spend the first 10-15 minutes of most class periods on warmup activities. You’re expected to have prepared the warmup activity ahead of time (this means you’ll need to have completed the readings as well). Each time, we’ll sort into groups of 5-6 students, and one of you (randomly selected) will be responsible for presenting the activity on the whiteboard. If you’re not feeling prepared to present the activity, you can “pass” to the next person, or ask for help along the way.\nI aspire to do all of the readings – including some optional theory readings – as well as all of the warm-ups to the best of my ability. It doesn’t feel productive to place a maximum number of “passes” because I trust my effort and intentions in doing my best work; instead, I’ll try to attend student hours if I get stumped. I might need help from classmates sometimes, and I think that’s ok.\nBefore class, I aspire to preview the in-class coding we’ll be doing so that I prep myself well for understanding the content.\nI have already begun working with some classmates outside of class, so I hope to sometimes be the one to organize work times.\nSomething I want to work on is my presentation skills. I presented on an algorithm for my J-Term class, and I felt poorly about how it went. Each time I’m chosen to present the warm-up in ML, I want to treat it as practice speaking in front of others and engaging my peers in discussion. I’m not one to speak often in class, but I aspire to ask a few questions over the semester and maybe even answer some questions. More practical for me, perhaps, is to challenge myself to attend student hours to ask specific questions when I get stuck.\n\n\n\nTo finish off the course, you’ll complete a long-term project that showcases your interests and skills. You’ll be free to propose and pursue a topic. My expectation is that most projects will move significantly beyond the content covered in class in some way: you might implement a new algorithm, study a complex data set in depth, or conduct a series of experiments related to assessing algorithmic bias in a certain class of algorithm. You’ll be expected to complete this project in small groups (of your choosing), and update us at a few milestones along the way.\nPlease share a bit about what kind of topic might excite you, and set a few goals about how you plan to show up as a constructive team-member and co-inquirer (see the ideas for some inspiration).\nI look forward to working on a culmianting project with some classmates! Right now, I think I’ll be interested in finding a complex dataset and applying some of the techniques we’ve learned in class as well as trying some new techniques. At the end of the project, I want to feel as though I “pulled my weight” with technical contributions, but also practice my interpersonal teamwork skills and hold teammates accountable for work. The final presentation will be a great opportunity for me to apply what I’ve been practicing in leading warm-ups."
  },
  {
    "objectID": "posts/WhoseCosts/index.html",
    "href": "posts/WhoseCosts/index.html",
    "title": "Whose Costs?",
    "section": "",
    "text": "Whose Costs?\n\nIntroduction\nThis blog tackles a real world problem: given information on an individual and their specific loan request, should a bank offer this person a loan? I will conduct an exploration of the data to determine which features in the dataset are the best predictors of a person’s loan status (whether they will default on their loan, or not). Using logistic regression, I will determine a weight vector to use in a linear score function. Then, with the only goal of maximizing profit, I will choose a threshold value: individuals with scores below this value will be offered a loan by the mythical Liz Bank, and those with scores above this value will be denied. However, a model created with the sole goal of maximizing profit is bound to make harsh judgements and marginialize groups. At the end of this post, I’ll evaluate to what extent this is true.\n\n# import relevent packages\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nFirst, let’s grab the data. Our dataset has 12 features and 22,907 entries. Our predictor variable is loan_status: a 1 indicates that this person defaulted on the loan and the bank lost money (bad!), and a 0 means they didn’t default.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\ndf_train = df_train.dropna()\ndf_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10\n\n\n6\n21\n21700\nRENT\n2.0\nHOMEIMPROVEMENT\nD\n5500\n14.91\n1\n0.25\nN\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\nMORTGAGE\n8.0\nEDUCATION\nA\n3000\n7.29\n0\n0.02\nN\n17\n\n\n26060\n23\n48000\nRENT\n1.0\nVENTURE\nA\n4325\n5.42\n0\n0.09\nN\n4\n\n\n26061\n22\n60000\nRENT\n0.0\nMEDICAL\nB\n15000\n11.71\n0\n0.25\nN\n4\n\n\n26062\n30\n144000\nMORTGAGE\n12.0\nPERSONAL\nC\n35000\n12.68\n0\n0.24\nN\n8\n\n\n26063\n25\n60000\nRENT\n5.0\nEDUCATION\nA\n21450\n7.29\n1\n0.36\nN\n4\n\n\n\n\n22907 rows × 12 columns\n\n\n\n\n\nExplore Data\nLet’s make a summary table and 2 visualizations to explore our data. In this process, we want to begin to determine which features may be good predictors of loan status.\nFirst, let’s look at the mean loan amount for different home ownership statuses. Unsurprisingly, people with mortgages had a high mean loan amount of ~10,600. This makes sense, as their loan may be the mortgage itself, or these individuals’ lack of disposable income may lead them to take loans for other purposes as well. People who rent and own houses had lower mean loan amounts around $8,000 - $9,000.\n\ntable = df_train.groupby(['person_home_ownership']).agg(\"loan_amnt\").mean().reset_index()\ntable\n\n\n\n\n\n\n\n\nperson_home_ownership\nloan_amnt\n\n\n\n\n0\nMORTGAGE\n10620.177719\n\n\n1\nOTHER\n11305.194805\n\n\n2\nOWN\n9051.955782\n\n\n3\nRENT\n8912.191822\n\n\n\n\n\n\n\nNext, I made a scatterplot illustrating loan status depending on loan interest rate and loan amount. From the graph, I was able to infer a decision boundary for loan interest rate: &gt;12.5% and people tend to default on their loans, &lt;12.5% and people tend not to. Curiously, it was challenging to see a pattern when considering loan amount.\n\nsns.set_palette(\"dark\")\nplot1 = sns.scatterplot(df_train, \n                        x = \"loan_int_rate\", \n                        y = \"loan_amnt\", \n                        hue = \"cb_person_default_on_file\", \n                        size = 0.5).set(xlabel = \" Loan Interest Rate \",\n                                        ylabel = \" Loan Amount\",\n                                        title = \"Defaults based on Loan Interest Rate and Loan Amount\")\n\nplt.legend(title = \"Default on File\")\n\nplot1\n\n[Text(0.5, 0, ' Loan Interest Rate '),\n Text(0, 0.5, ' Loan Amount'),\n Text(0.5, 1.0, 'Defaults based on Loan Interest Rate and Loan Amount')]\n\n\n\n\n\n\n\n\n\nFinally, these histograms helped me understand how purpose of loans change with age. Unsurprisingly, people in their twenties tend to take out loans for education, with people in their late twenties making venture loans to start businesses. With age, loans transition for medical and personal purposes.\n\nfig, axs = plt.subplots(3,2)\nplt.title(\"Distribution of Loan Intent by Age\")\nplt.subplots_adjust(hspace = 0.9)\n\nsns.histplot(data = df_train[df_train['loan_intent'] == \"VENTURE\"], x = \"person_age\", color=\"skyblue\", ax = axs[0,0], bins = 60).set(title = \"Venture\", ylim = (0,2000), xlim = (20,60), xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"EDUCATION\"], x = \"person_age\", color=\"olive\", ax = axs[1,0], bins = 60).set(title = \"Education\", ylim = (0,2000), xlim = (20,60), xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"MEDICAL\"], x = \"person_age\", color=\"gold\", ax = axs[2,0], bins = 35).set(title = \"Medical\", ylim = (0,2000), xlim = (20,60), xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"HOMEIMPROVEMENT\"], x = \"person_age\", ax = axs[0,1], bins = 25).set(title = \"Home Improvement\", ylim = (0,2000), xlim = (20,60), ylabel = None, xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"PERSONAL\"], x = \"person_age\", color=\"teal\", ax = axs[1,1], bins = 60).set(title = \"Personal\", ylim = (0,2000), xlim = (20,60), ylabel = None, xlabel = \"age\")\nsns.histplot(data = df_train[df_train['loan_intent'] == \"DEBTCONSOLIDATION\"], x = \"person_age\", ax = axs[2,1], bins = 25).set(title = \"Debt Consolidation\", ylim = (0,2000), xlim = (20,60), ylabel = None, xlabel = \"age\")\n\nfig.suptitle(' Distribution of Loan Intent by Age ', fontsize=20)\nplt.show()\n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\n\n\nBuild a Model\nNow it’s time to build a model. First, one-hot-encode the qualitative columns in the training set and drop features not permitted in the mdoel (loan status – the predictor, and loan grade)\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# before one-hot-encoding, make copy of data for later visualization\ndf_for_viz = df_train.copy()\n\n# label-encode qualitative features\nle = LabelEncoder()\nfor label in [\"loan_intent\", \"cb_person_default_on_file\"]:\n    df_train[label] = le.fit_transform(df_train[label])\n\n# manually one-hot encode to force more logical order\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OTHER', 0)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('RENT', 1)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('MORTGAGE', 2)\ndf_train['person_home_ownership'] = df_train['person_home_ownership'].replace('OWN', 3)\n\ny_train = df_train[\"loan_status\"]\nX_train = df_train.drop(columns=['loan_status', 'loan_grade'])\nX_train\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_amnt\nloan_int_rate\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n1\n27\n98000\n1\n3.0\n1\n11750\n13.47\n0.12\n1\n6\n\n\n2\n22\n36996\n1\n5.0\n1\n10000\n7.51\n0.27\n0\n4\n\n\n3\n24\n26000\n1\n2.0\n3\n1325\n12.87\n0.05\n0\n4\n\n\n4\n29\n53004\n2\n2.0\n2\n15000\n9.63\n0.28\n0\n10\n\n\n6\n21\n21700\n1\n2.0\n2\n5500\n14.91\n0.25\n0\n2\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n26059\n36\n150000\n2\n8.0\n1\n3000\n7.29\n0.02\n0\n17\n\n\n26060\n23\n48000\n1\n1.0\n5\n4325\n5.42\n0.09\n0\n4\n\n\n26061\n22\n60000\n1\n0.0\n3\n15000\n11.71\n0.25\n0\n4\n\n\n26062\n30\n144000\n2\n12.0\n4\n35000\n12.68\n0.24\n0\n8\n\n\n26063\n25\n60000\n1\n5.0\n1\n21450\n7.29\n0.36\n0\n4\n\n\n\n\n22907 rows × 10 columns\n\n\n\nWhat number of features, and which ones, are most effective in a model to predict loan status? To answer this question, I turned to logistic regression. Utilizing the combinations function from itertools, I fit a logistic regression model with 5 cross-validation batches for each combination of features. I began with NUMFEATURES = 2 to test couples of features and found that person_home_ownership and loan_percent_income produced the highest score of 0.849. Adjusting NUMFEATURES to 3 and 4 produced highest scores of 0.484, so I deduced that those two features to be the simplest and most effecetive.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\npd.set_option('max_colwidth', 10000)\nNUMFEATURES = 2\n\ncols = X_train.columns\n\nscore_df_columns = ['features', 'score', 'weights']\nscore_df = pd.DataFrame(columns = score_df_columns)\n\nfor combo in combinations(cols, NUMFEATURES):\n  combo = list(combo)\n  \n  # Logistic Regression\n  LR = LogisticRegression(max_iter = 20000000000)\n  LR.fit(X_train[combo], y_train)  \n  LRscore = cross_val_score(LR, X_train[combo], y_train, cv = 5).mean()\n  score_df.loc[len(score_df.index)] = [combo, LRscore, LR.coef_]  \n\nscore_df = score_df.sort_values(by='score', ascending=False).head(10)\nscore_df\n\n\n\n\n\n\n\n\nfeatures\nscore\nweights\n\n\n\n\n21\n[person_home_ownership, loan_percent_income]\n0.848736\n[[-1.0098629403833108, 8.271824027284113]]\n\n\n39\n[loan_int_rate, loan_percent_income]\n0.823416\n[[0.2896471726415302, 8.417607957665085]]\n\n\n32\n[loan_intent, loan_percent_income]\n0.819269\n[[-0.1108135938796114, 8.2548348300199]]\n\n\n27\n[person_emp_length, loan_percent_income]\n0.819138\n[[-0.04604467360459407, 8.17718472200731]]\n\n\n6\n[person_age, loan_percent_income]\n0.815471\n[[-0.0058903490094554785, 8.195442809295034]]\n\n\n43\n[loan_percent_income, cb_person_cred_hist_length]\n0.815340\n[[8.201924752324944, -0.007641209561773739]]\n\n\n20\n[person_home_ownership, loan_int_rate]\n0.814292\n[[-0.9816330580330728, 0.2767376475314839]]\n\n\n42\n[loan_percent_income, cb_person_default_on_file]\n0.813769\n[[8.373163550192576, 1.0910615242300115]]\n\n\n12\n[person_income, loan_amnt]\n0.808006\n[[-4.057354027170065e-05, 0.0001065591015240086]]\n\n\n31\n[loan_intent, loan_int_rate]\n0.803990\n[[-0.10705093453352543, 0.27916038916611374]]\n\n\n\n\n\n\n\nFrom the LogisticRegression.coef_ features, I was able to find the weights to use in my score function.\n\nweights = score_df.iloc[0][\"weights\"][0]\nweights\n\narray([-1.00986294,  8.27182403])\n\n\n\n\nFind a Threshold\nExtract the two desired features for the X_traindataset, and set the y_train dataset to be the predictor variable – loan_status\n\nX_train = df_train[['person_home_ownership', 'loan_percent_income']]\ny_train = df_train['loan_status']\n\nBefore we continue with making our model, let’s quickly check if it makes sense that these two variables are effective in modeling loan status. It turns out that yes, we see clear differences in loan_percent_income in the different person_home_ownership groups for those who did and didn’t default on their loans.\n\nsns.set_palette(\"dark\")\nplot1 = sns.barplot(df_for_viz, \n                        x = \"person_home_ownership\", \n                        y = \"loan_percent_income\", \n                        hue = \"loan_status\")\n\nplt.legend(title = \"Default on File\")\n\nAttributeError: 'numpy.int64' object has no attribute 'startswith'\n\n\n\n\n\n\n\n\n\nThe linear score for each entry of X_train is just X_train @ weights, where @ represents matrix multiplication.\n\ndef linear_score(X, w):\n    return X@w\n\ns = linear_score(X_train, weights)\n\nLet’s take a look at the scores we produced. All scores are between -3 and 5, with most being around 0. The distribution of scores roughly takes the form of a bell curve.\n\nhist = plt.hist(s)\nlabs = plt.gca().set(xlabel = r\"Score $s$\", ylabel = \"Frequency\") \n\n\n\n\n\n\n\n\nWhat should the threshold be to maximize profit? To answer this question, I chose to copy df_train into df_profit. This way, I could add a column y_pred, which is False if the person’s score is less than the threshold and True if it is greater than or equal to the threshold. With a for loop, different values of threshold t can be considered. Another column, outcome, labels entries as true negatives, false negatives, false positives, and true positives. That way, each person’s profit contribution to the bank can be stored in a final column: profit. True positives and false positives don’t contribute any profit (neither positive nor negative) because they won’t be offered a loan in the first place – these people are expected to default. To find the average profit per loan, divide by the combined number of false negatives and true negatives\n\ndf_profit = df_train\n\nT = np.linspace(s.min()-0.1, s.max()+0.1, 101)\nprofits = []\n\nfor t in T:\n\n    y_pred = (s &gt;= t)\n\n    df_profit['y_pred'] = y_pred\n    df_profit['outcome'] = \"empty\"\n    df_profit['profit'] = 0.0\n\n    # label entries as true/false negative/positive\n    df_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 0), 'outcome'] = 'TN' # gain money\n    df_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 1), 'outcome'] = 'FN' # lose money\n    df_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 0), 'outcome'] = 'FP' # won't give loan\n    df_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 1), 'outcome'] = 'TP' # won't give loan\n    \n    # true positive -- bank makes money!\n    df_profit.loc[df_profit['outcome'] == 'TN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**10 - (df_profit['loan_amnt'])\n\n    # False negative -- bank loses money\n    df_profit.loc[df_profit['outcome'] == 'FN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**3 - (1.7*df_profit['loan_amnt'])\n\n    # total numbers of false negatives and true negatives, to calculate avg profit/ accepted loan (pred = False)\n    numFN = df_profit.loc[df_profit['outcome'] == 'FN'].size\n    numTN = df_profit.loc[df_profit['outcome'] == 'TN'].size\n    \n    if (numFN + numTN != 0):\n        totalprof = sum(df_profit['profit']) / (numFN + numTN)\n        profits.append(totalprof)\n    else:\n        profits.append(0)\n\nPlotting the average profit per loan results for each threshold value shows a maximum when threshold = -2. The expected profit per loan is around $110 at this value.\n\nimport matplotlib.ticker as ticker\n\nplt.plot(T, profits)\nplt.gca().set(ylim = (0, 120), xlim = (-4, 4))\nlabs = plt.gca().set(xlabel = r\"Threshold $t$\", ylabel = \"Expected profit per loan\")\nplt.grid(True)\n\n\n\n\n\n\n\n\n\n\nEvalute your Model from the Bank’s Perspective\nLet’s run our model on the test set, with a threshold value of -2.\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/test.csv\"\ndf_test = pd.read_csv(url)\ndf_test =df_test.dropna()\n\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OTHER', 0)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('RENT', 1)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('MORTGAGE', 2)\ndf_test['person_home_ownership'] = df_test['person_home_ownership'].replace('OWN', 3)\n\n\nX_test = df_test[['person_home_ownership', 'loan_percent_income']]\n\n\n\n\n\n\n\n\nperson_home_ownership\nloan_percent_income\n\n\n\n\n0\n1\n0.02\n\n\n1\n2\n0.29\n\n\n2\n1\n0.06\n\n\n3\n2\n0.15\n\n\n4\n1\n0.08\n\n\n...\n...\n...\n\n\n6511\n2\n0.23\n\n\n6513\n1\n0.29\n\n\n6514\n3\n0.22\n\n\n6515\n2\n0.09\n\n\n6516\n1\n0.16\n\n\n\n\n5731 rows × 2 columns\n\n\n\n\nt = -2\ns_test = linear_score(X_test, weights)\ny_pred_test = (s_test &gt;= t)\n\n\ndf_profit = df_test.copy()\n\ndf_profit['y_pred'] = y_pred_test\ndf_profit['outcome'] = \"empty\"\ndf_profit['profit'] = 0.0\n\ndf_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 0), 'outcome'] = 'TN' # gain money\ndf_profit.loc[(df_profit['y_pred'] == False) & (df_profit['loan_status'] == 1), 'outcome'] = 'FN' # lose money\ndf_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 0), 'outcome'] = 'FP' # won't give loan\ndf_profit.loc[(df_profit['y_pred'] == True) & (df_profit['loan_status'] == 1), 'outcome'] = 'TP' # won't give loan\n    \n# true positive -- bank makes money!\ndf_profit.loc[df_profit['outcome'] == 'TN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**10 - (df_profit['loan_amnt'])\n\n# False negative -- bank loses money\ndf_profit.loc[df_profit['outcome'] == 'FN', 'profit'] = df_profit['loan_amnt'] * (1 + (0.25 * (df_profit['loan_int_rate'] / 100)))**3 - (1.7*df_profit['loan_amnt'])\n\nnumFN = df_profit.loc[df_profit['outcome'] == 'FN'].size\nnumTN = df_profit.loc[df_profit['outcome'] == 'TN'].size\n    \ntotalprofit = sum(df_profit['profit']) / (numFN + numTN)\n\nprint(totalprofit)\n\n139.52875179012145\n\n\nThe expected profit per borrower is ~$140. This is $30 larger, or 27% larger, than the average profit per loan on the test set.\n\n\nEvaluate model from Borrower’s Perspective\nOur model produces great profits for the bank. But how does it function from the borrower’s perspective?\nThe following table shows one thing very clearly: this model does NOT grant loans to many individuals. In order to maximize the profit per loan, the model only predicts people to not default if they have a score &lt;-2. Considering the histogram of scores from earlier, we know that a very small subset of people in our training set had scores &lt;-2. In our test set, that’s only 174 people out of the total 5731.\n\ndf_profit.groupby('y_pred').size()\n\ny_pred\nFalse     174\nTrue     5557\ndtype: int64\n\n\nLet’s consider: is it more difficult for people in certain age groups to access credit? The following histogram indicates that no, our model accepts people from different age groups at equal rates.\n\n\nfig, axs = plt.subplots(1,2)\nplt.title(\"Distribution of Loan Intent by Age\")\nplt.subplots_adjust(hspace = 0.9)\n\n# is it more difficult for people in certain age groups to access credit?\nsns.histplot(data = df_profit, x = \"person_age\", hue = \"y_pred\", bins = 25, edgecolor = 'grey', ax = axs[0]).set(xlabel = \"person age\", ylabel = \"count\")\nsns.histplot(data = df_profit[df_profit['y_pred'] == False], x = \"person_age\", hue = \"y_pred\", bins = 25, edgecolor = 'grey', ax=axs[1])\n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nHowever, considering the following table, there are unfair trends. People 50+ are predicted to default 100% of the time and are consequently offered no loans by our model. It is easiest for people aged 40-50 to receive a loan; although they are expected to default 95.45% of the time, this is the lowest percentage of any age group.\n\nbins = [20, 30, 40, 50, 60, 70] \ndf_profit['Age Group'] = pd.cut(df_profit['person_age'], bins=bins, labels=['20-30', '30-40', '40-50', '50-60', '60-70'])\nsummary_stats = df_profit.groupby('Age Group').aggregate('y_pred').mean()\nsummary_stats\n\n/var/folders/nd/3yjvm85j3rq1vhh53yn6cy0r0000gn/T/ipykernel_63697/368891113.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  summary_stats = df_profit.groupby('Age Group').aggregate('y_pred').mean()\n\n\nAge Group\n20-30    0.969766\n30-40    0.970776\n40-50    0.954545\n50-60    1.000000\n60-70    1.000000\nName: y_pred, dtype: float64\n\n\nNow to consider: is it more difficult for people to get loans in order to pay for medical expenses? Medical loans are the 2nd most challenging to secure with our model, after Debt Consolidation.\n\n# predictions for default in groups\ndf_profit.groupby('loan_intent').aggregate('y_pred').mean()\n\nloan_intent\nDEBTCONSOLIDATION    0.997788\nEDUCATION            0.963435\nHOMEIMPROVEMENT      0.965909\nMEDICAL              0.971109\nPERSONAL             0.970942\nVENTURE              0.950207\nName: y_pred, dtype: float64\n\n\nWhen compared to the actual rate of default, people in the test set defaulted on debt consolidation loans 28.76% of the time, and defaulted on medical loans 28.43% of the time.\n\n# actual rate of default:\ndf_profit.groupby('loan_intent').aggregate('loan_status').mean()\n\nloan_intent\nDEBTCONSOLIDATION    0.287611\nEDUCATION            0.167517\nHOMEIMPROVEMENT      0.250000\nMEDICAL              0.284250\nPERSONAL             0.220441\nVENTURE              0.146266\nName: loan_status, dtype: float64\n\n\nFinally, I am curious how a person’s income level impacts the ease with which they can access credit under my decision system. To answer this question, I made the following table which shows the percentages of people in each income group who would be granted a loan by my algorithm. Unfortunately, only 2.38% of people making &lt;$50,000 would be offered a loan by my model, whereas people making $150,000 - 200,000 would be offered a loan 5.13% of the time. People making $200,000+ would be granted loans at a much higher rate: 9.19%.\n\n# percentages of people predicted not to default\n\n# Define the income groups\nbins = [0, 50000, 100000, 150000, 200000, np.inf]\nlabels = ['0-50,000', '50,000-100,000', '100,000-150,000', '150,000-200,000', '200,000+']\n\n# Create a new column for income groups\ndf_profit['income_group'] = pd.cut(df_profit['person_income'], bins=bins, labels=labels)\n\n# Group by income_group and y_pred and count the number of rows\ngrouped = df_profit.groupby(['income_group', 'y_pred']).size()\n\n# Calculate the percentage of rows where y_pred is False in each income group\npercentage = grouped.xs(False, level='y_pred') / grouped.groupby(level='income_group').sum() * 100\n\nprint(percentage.reset_index())\n\n      income_group         0\n0         0-50,000  2.378073\n1   50,000-100,000  3.041216\n2  100,000-150,000  4.570384\n3  150,000-200,000  5.128205\n4         200,000+  9.195402\n\n\n/var/folders/nd/3yjvm85j3rq1vhh53yn6cy0r0000gn/T/ipykernel_63697/537452620.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped = df_profit.groupby(['income_group', 'y_pred']).size()\n/var/folders/nd/3yjvm85j3rq1vhh53yn6cy0r0000gn/T/ipykernel_63697/537452620.py:14: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  percentage = grouped.xs(False, level='y_pred') / grouped.groupby(level='income_group').sum() * 100\n\n\n\n\nConclusion\nIn this exercise, I blindly set out with one goal: maximizing profit. I intentionally put my opinions aside to determine what the outcome would be if I truly just tried to make a model that makes the most money for a bank. Consequently, I used the “best” features for modeling loan status and set a very low threshold value that offered loans to very very few people – only ones rated very low chance of default. These people, unsurprisingly, tended to have high income, be in their 30s and 40s, and be requesting loans for venture use.\nOk, so I’ve made this brutal algorithm, now I can finally bring my brain back into the equation and think about what I’ve done. The people the mythical Liz Bank is offering loans to… don’t NEED the loans as badly as the others. They’re already rich! This hardly feels fair – people who have more money to begin with can get loans for more money, while others can’t?\nNow I need to describe how I like to consider fairness. I like thinking about it in terms of equality vs. equity. In an equal world, everyone is treated the same no matter their circumstances. In an equitable world, people with less advantages are given a “boost” so that people have near equal opportunities. I think that a fair world is an equitable world, but not necessarily an equal world.\n\n\n\ncitation:https://mostly.ai/blog/we-want-fair-ai-algorithms-but-how-to-define-fairness\n\n\nThis was a valable exercise. My mom works on a loan committee at a small local bank, and now I am curious how she and her colleagues determine who to grant loans to in our community. I am excited to ask her more about this, any models they use in their decisions, and maybe offer her some new perspective on the issue. This exercise reminded me of the importance of staying connected to the problem I’m solving with ML: things get dicey when there’s only one goal in mind (e.g: maximize profit!)"
  },
  {
    "objectID": "posts/WomenInDataScience/index.html",
    "href": "posts/WomenInDataScience/index.html",
    "title": "Women in Data Science Conference",
    "section": "",
    "text": "Women in Data Science Conference\n\nAbstract\nWomen are underrepresented in computing, math, and engineering, making up just 26% of computing professionals and 12% of engineering professionals in 2013. In this blog post, I aim to investigate how this gender divide developed, consider techniques proven to reverse this disparity, and apply my learning to the Middlebury College Women in Data Science Conference.\n\n\nIntroduction\nFemale underrepresentation in computing is problematic for many reasons. First, group dynamics in teams without women suffer, as women are “more likely than men to prioritize helping and working with other people over career goals” (Corbett & Hill, 4). The female perspective brings diversity to the workforce, contributing to creativity, productivity, and innovation. The biggest issues of this century – climate change, disease, overpopulation – require the skills of engineers and computer scientists. Everyone misses out on novel solutions when diverse participation is not prioritized. Underrepresentation of women in engineering fields is becoming a global issue, too; the US is less globally competitive because it is not hiring the best people for the jobs.\nIt wasn’t always the case that female voices were scarce in these technical fields; women were significant players in the early decades of computing. Ada Lovelace was one of the early pioneers in the 1880s, and numerous women were programmers during World War II (Abbate). In the 1950s and 1960s, women maintained a presence in computing — representing about a quarter of workers. This statistic grew steadily until 1990, when just over a third of computing workers were women. At this time, computing lacked a gender identity and attracted women as well as men. However, women made up just 26% of computing professionals in 2013. “Over a relatively short period of time, a field that was once relatively gender-integrated has become solidly male-dominated” (Koput & Gutek, 105). What caused a decrease in women’s representation from 1990? One explanation lies in hiring practices that favored men, such as aptitude and personality tests which privileged male stereotyped characteristics — antisocial, mathematically inclined programmers were considered to be the best. Also, as personal computers became more readily available, young boys gained dominant ownership over these toys and the association of men and computers was fostered. As programming curriculums became more lucrative at colleges, universities imposed stringent requirements for entry and completion of the major, disadvantaging women who entered college with less programming experience after being encouraged to study things like “home economics” if they were found to be engineering-minded (Bix, 2002).\nHowever, the representation of women in computing can improve and is improving. At Harvey Mudd College, the percentage of women graduating from computing grew from 12% to 40% in five years, brought about by a revision of the introductory course, research opportunities for undergraduates, and taking female students to a Women in Computing Conference (Corbett & Hill, 4). There are many reasons why hosting events that spotlight women in STEM is critical for female engineering representation. Women in engineering and computing often report isolation, a lack of voice, and a lack of support, whereas men cite ample social networks in computing (Corbett & Hill, 34). It is important to create spaces that allow women to network and support each other, as a sense of belonging has measurable effects on an individual’s physical and mental state. The stereotype that women are less competent programmers is incredibly dangerous. Stereotype bias inhibits women when “individuals fear that they will confirm a negative stereotype about a group to which they belong” (Corbett & Hill, 3). These harmful stereotypes make women less confident and therefore hurt their performance. In the modern decade, explicit gender bias is relatively rare; what is more problematic is implicit gender bias, which can be combated only by individuals of all genders attending events that spotlight women in STEM.\n\n\nConference Speakers\nAn example of such an event is the Women in Data Science Conference at Middlebury College. This year, in 2024, three female professors from different academic departments presented lightning talks on how they use data science in their research projects, three female alumni spoke on their unique careers in data science, and Professor Sarah Brown visited from the University of Rhode Island to speak on her research concerning fairness in machine learning.\nThe first lightning speaker was Professor Laura Beister from the computer science department, who presented her Ph.D. research. Professor Beister uses natural language processing to analyze Reddit posts of individuals who self-identify themselves online as depressed. This research analyzes posts on Reddit; while this may seem unreliable, it is true that people tend to post anonymously on Reddit about their conditions, known as an “identity claim,” and be truthful in such posts. Reddit is an incredibly large body of text, so Professor Beister used heavy computing power to search through the postings, find relevant ones, and then convert them into vectorized representations that she could perform analysis. She found that depressed individuals’ language markers are similar to those of anxious, sad individuals, but more closely match controls (non-depressed individuals) after they make an identity claim as a depressed individual online. This indicates that there may be benefits to sharing one’s depression diagnosis, especially in a semi-anonymous form where people are likely to be empathetic.\nNext, Professor Amy Yuen presented her work modeling behavior in the UN Security Council to examine when the council decides to take up an issue and how the institution affects policy coordination. I learned that the UN Security Council is one of the six principal components of the United Nations and is tasked with recommending the admission of new UN members to the General Assembly and approving any changes to the UN Charter. Amy Yuen gathered data from different actors in the UN Security Council, a time-intensive process that relied heavily on data cleansing. She then used data science to argue for a different approach to considering the council’s decisions: a legislative bargaining framework. While I struggled to understand some of the principles surrounding international conflict resolution, I came out of this talk with an appreciation for how gathering and cleansing data can prove to be the most challenging, time-intensive part of the research process, and I began thinking about the skills I’ve learned in CS classes and how they may aid in this process.\nThe last lightning speaker was Professor Jessica L’Roe, who spoke on one of her research projects concerning land access and trends of inequality in the highlands of East Africa. One of her research projects that I found particularly interesting surveyed fifty rural women about their strategies and challenges investing in children’s future livelihoods via land. She found that over 80% of women thought investing in education was more worthwhile than land. I was inspired to learn more about her path to becoming a woman in geography research, and was appreciated her reflection on gender stereotypes that she came to reject later in her life. She also touched on the challenge of gathering data in developing countries, where the results of the research are celebrated in the academic world but may have a relatively low impact on the communities they analyze.\nFinally, the visiting speaker, Dr. Professor Sarah Brown, gave an in-depth talk about her path to becoming a professor of data science. She described machine learning as a combination of computer science, statistics, and domain expertise; a description I’d never heard before. Professor Brown emphasized the importance of understanding problems in a nuanced way by soliciting the advice of industry professionals before trying to apply data science. A complex understanding of bias and consideration of fairness must occur before a data scientist ever considers building a model. This is why Professor Sarah Brown designed her curriculum to teach students about fairness before ever teaching them how to fit a machine learning model. Professor Brown’s path to professorship was interesting and dynamic. In her career, she has learned “keys” that help her succeed. The first is understanding data’s context. She shared an anecdote about how diving deeply into a dataset allowed her to modify a regression problem to include classification in the cost function, producing better results for PTSD diagnosis. Next, a data scientist must consider disciplines as communities. Understanding data and a problem requires soliciting advice from people in all fields. Finally, Professor Brown spoke on the importance of meeting people where they are. A data scientist must be able to present technical findings to a non-technical audience, and explaining complex models in a flamboyant way only reduces its impact.\n\n\nConclusion\nI found it valuable to learn more about the presence of gender disparities in a field I so passionately want to break into, and then take this knowledge to the Women in Data Science Conference. While it was at times upsetting to learn about the deep-rooted history of these inequalities, I now have a calm understanding of the facts that I can leverage to support myself if I ever find myself in a frustrating situation. I also now know techniques that can be used to reverse the statistics, of “solve the equation.” I’m inspired by the incredible work of women in data science that was showcased at this event, and I aspire to someday join the ranks of bright women in this field."
  },
  {
    "objectID": "posts/LimitsToQuantitativeApproach/LimitsToFairness.html",
    "href": "posts/LimitsToQuantitativeApproach/LimitsToFairness.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "Limits to the Quantitative Approach to Bias and Fairness\n\nIntroduction\nWe live in the era of “Big Data.” Information regarding our individual movements and tendencies is constantly being collected via digital devices, sensors, and online platforms. This data takes various forms, be it traditional databases, semi-structured formats like JSON, and unstructured data from sources like social media. Perhaps unsurprisingly, the pace with which this data is processed is constantly increasing – nearing real-time. Endless data is publicly available; massive datasets can be downloaded from portals or through APIs for any analytics use. Terabytes of data are funneled into machine learning algorithms which make decisions and offer recommendations for the common person, whether or not they’re aware of it. Companies, policymakers, and computer scientists alike rave about the efficiency and seemingly endless possibilities provided by big data and big data analytics.\nAs algorithms started being used for decisions with significant outcomes, critics began to recognize the potential for these algorithms to become biased against individuals of certain identity groups, causing additional harm. Famously, algorithms used by the police force to predict where crimes may occur and algorithms used by the justice department to predict the probability of a defendant re-committing a crime were proven to be rampantly biased against people of color. Individuals concerned with the ethical implications of data-driven decisions sparked a movement countering Big Data, pushing for metrics of fairness in algorithms. Now, a framework for verifying fairness is common practice, relying on various quantitative methods. Researchers can use statistical definitions of fairness as a crutch to support their algorithmic findings, but it still remains uncommon (Kordzadeh and Ghasemaghaei (2022)).\n\n\nDefinitions of Fairness\nThere are three main quantitative definitions of fairness. First is error rate parity, in which all groups experience the same false negative rate and false positive rate. This means that whether a person is a member of Group A or Group B, the algorithm is equally likely to make a mistake. The next is acceptance rate parity, in which an algorithm equalizes acceptance rates across all groups. The outcome of an algorithm should be independent of group membership. Finally is sufficiency, in which the probability of seeing a positive outcome given a positive prediction is the same for all subgroups. Similarly, the probability of seeing a negative outcome given a negative prediction must also be the same for all subgroups (Barocas, Hardt, and Narayanan (2023)).\nNow, consider the moral standpoint, which also contains three “views” from which one can consider fairness: a narrow, middle, and broad view. In the narrow view, people similar with respect to a task should be treated similarly. Comparison should be between all people as individuals, not between members of specific groups. The middle view begs decision-makers to uphold an obligation to avoid perpetuating injustice; they must treat seemingly dissimilar people similarly when the causes of those differences are themselves problematic. Finally, the broad view of equality hopes that people with similar abilities and ambitions could achieve similar successes, despite inevitable inequities. This view isn’t really about fairness in decision-making, it’s about the design of society’s basic institutions, with the goal of preventing unjust inequalities from arising in the first place (Barocas, Hardt, and Narayanan (2023)).\n\n\nThe Utility of Quantitative Methods\nOne example of where quantitative and moral definitions of fairness are effectively leveraged to critique an algorithm appears in a study performed by Ali et al. titled “Discrimination through Optimization: How Facebook’s Ad Delivery Can Lead to Biased Outcomes.” In this study, the authors consider Facebook’s ad targeting algorithm, hoping to determine if it targets job advertisements differently for men and women. Facebook states, “we try to show people the ads that are most pertinent to them,” In doing this, the researchers wonder, does Facebook make gender or racial generalizations? The team created generic job posting advertisements for eleven different roles: AI developer, doctor, janitor, lawyer, lumberjack, nurse, preschool teacher, restaurant cashier, secretary, supermarket clerk, and taxi driver. The advertisements were submitted to Facebook and the researchers paid for them to be “live” for 24 hours. Then, the research team collected ad delivery data and broke the results along gender, age, and race lines. They observed dramatic differences in ad delivery in different racial and gender groups. “Our ads for positions in the lumber industry deliver to over 90% men and to over 70% white users in aggregate, while our ads for janitors deliver to over 65% women and over 75% black users in aggregate” (Ali et al. (2019)).\nHere, the researchers proved that Facebook’s algorithm did not uphold the acceptance rate parity. The probability of receiving a lumber industry advertisement should be independent of gender, and the probability of receiving a janitorial advertisement should be independent from race, yet neither of these is the case. As far as non-technical definitions of fairness, the research team claimed that the algorithm was unfair considering the middle view. Facebook has an obligation to treat dissimilar people (men and women) similarly when the causes of the dissimilarities are themselves problematic. In this case, it is problematic to assume that men are more interested in lumberjack and AI roles than women, even if it is the case that more men click on these advertisements than women. Therefore, Facebook must consider adjusting its advertising scheme so as not to perpetuate gender stereotypes in labor. This excellent study acts as an optimistic example of the utilitarianism of quantitative metrics to shine a light on unintentional algorithmic bias.\n\n\nA Note of Caution: Arvind Naryanan\nHowever, these quantitative measures of fairness aren’t enough to claim an algorithm’s innocence, claims Arvind Narayanan, a computer scientist and professor at Princeton University. On October 11th, 2022, he gave a talk titled “The Limits of the Quantitative Approach to Discrimination,” which provides a timely and fresh criticism of quantitative measures of fairness. His central claim is: “Currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (Narayanan (2022)).\nNarayanan begins his speech by explaining that “all models are wrong, but some models are useful” (Narayanan (2022)). No machine learning model makes the right prediction every time; the simplification of trends necessary to fit a clean model introduces bias. In such generalizations, models feast on the status quo, engraining generalities and failing to treat them as problematic. However, we know that the way our world operates is extremely problematic; our reality is replete with discrimination. Therefore, Narayanan believes that “data aren’t inert and objective. They are political, and produced towards certain ends” (Narayanan (2022)). When data are collected for a certain purpose, there is inevitable bias baked into them. This notion is summarized well by Narayanan and co-authors Barocas and Hardt in their book Fairness and Machine Learning: Limitations and Opportunities. In the introduction, the authors explain that “the state of the world is reduced to a set of rows, columns, and values in the dataset. It’s a messy process, because the real world is messy. The term measurement is misleading, evoking an image of a dispassionate scientist recording what she observes, yet …it requires subjective human decisions.” (Barocas, Hardt, and Narayanan (2023)). There’s a popular notion that data doesn’t lie and a conclusion drawn from data must be fact. However, Narayanan’s concept of data explains it not as truth but as a concentrated soup of the biases, discriminations, and unequal opportunities present in the real world. To combat this characteristic of data, Narayanan says, “we should be spending most of our time on curating and interrogating datasets before ever searching for statistical significance or fitting a model… it is important to look behind the facade of numbers to understand the hidden assumptions and politics of datasets” (Narayanan (2022)).\n\n\nDrawbacks to the Quantitative Method\nThere is truth to Narayanan’s claim; it isn’t challenging to find studies where quantitative methods are used to prove an algorithm’s fairness when in reality, it is biased. In a study titled Gender Earning Gap in the Gig Economy, Cook et al. set out to investigate if Uber’s algorithm is to blame for a 7% gender pay gap among rideshare drivers. The authors hone in on driver data from the metropolitan area of Chicago, and are able to find a statistical significance in the difference in means between the driving speeds of men and women and the time of day they choose to work. They also determine that men stay on the platform for a longer amount of time, causing them to be more experienced. Finally, they find a difference between neighborhoods where male and female drivers choose to drive. Therefore, the authors reason that the pay gap is due to these differences alone and not due to any bias in Uber’s algorithm. Yes, the results of the quantitative method of choice – statistical tests – is valid, but this is an incredibly simplistic way to view the issue. The authors would have done well to consider the middle view of equality, in which decision-makers have an obligation to avoid perpetuating injustice. There’s obvious injustice here: women are likely to drive in a smaller subset of neighborhoods and during daylight hours when wages are less because of the history of assault and mistreatment towards women, typically occurring at late hours and disproportionally in certain areas. Looking deeper into the study, it is revealed that many of the authors are Uber employees. It isn’t surprising, then, that they choose to leverage quantitative methods in a dishonest way. (Cook et al. (2021))\nThis study is a prime example of “Big Dick Data,” as defined by D’ignazio, Catherine and Klein in their book Data Feminism. “Big Dick Data is used to describe big data projects characterized by patriarchial, cis-masculinist, totalizing fantasizes of world domination as enacted through data capture analysis” (D’ignazio and Klein (2023)). Big Dick data projects are dangerous because they ignore context and inflate their technical and scientific capabilities. The authors of Data Feminism highlight the importance of asking questions about the social, cultural, historical, instutional, and material conditions around how data was collected, a process Ali et al. failed to do – miserably.\n\n\nConclusion\nNarayanan began his speech by saying, “I hope that this talk goes some way towards busting the myth that numbers don’t lie” (Narayanan (2022)). For me, at least, he succeeded in weakening my loyalty to data as capital-T Truth. I’ve realized that there is no way to collect a completely neutral, unbiased dataset, and therefore models always reflect the messy world we live in. You can just about always use quantitative methods to prove what you’re looking for, and that’s why a choice of null hypothesis is so important. In Ali et. al’s analysis of Uber’s algorithm, they set out to prove that gender bias wasn’t present, and unsurprisingly, they were able to prove such a thing using valid statistical claims.\nHowever, Narayanan goes so far as to say that quantitative methods do more harm than good and are too commonly used to justify racism. While I appreciate his stance, I still see the utility in quantitative methods; they can effectively be used to prove an algorithm to be racist or biased. It is true that there are so many quantitative definitions of fairness, meaning that under one definition, an algorithm can be proven to be biased, but under another, it can be proven to be fair. To bolster quantitative methods in a responsible, impactful way, we must not rely on just one quantitative definition of fairness and instead use them in combination with each other. Finally, we can’t just rely on numbers; it is important to bring the narrow, middle, and broad views of ethical fairness into every conversation.\n\n\n\n\n\n\nReferences\n\nAli, Muhammad, Piotr Sapiezynski, Miranda Bogen, Aleksandra Korolova, Alan Mislove, and Aaron Rieke. 2019. “Discrimination Through Optimization: How Facebook’s Ad Delivery Can Lead to Biased Outcomes.” Proceedings of the ACM on Human-Computer Interaction 3 (CSCW): 1–30.\n\n\nBarocas, Solon, Moritz Hardt, and Arvind Narayanan. 2023. Fairness and Machine Learning: Limitations and Opportunities. MIT Press.\n\n\nCook, Cody, Rebecca Diamond, Jonathan V Hall, John A List, and Paul Oyer. 2021. “The Gender Earnings Gap in the Gig Economy: Evidence from over a Million Rideshare Drivers.” The Review of Economic Studies 88 (5): 2210–38.\n\n\nD’ignazio, Catherine, and Lauren F Klein. 2023. Data Feminism. MIT press.\n\n\nKordzadeh, Nima, and Maryam Ghasemaghaei. 2022. “Algorithmic Bias: Review, Synthesis, and Future Research Directions.” European Journal of Information Systems 31 (3): 388–409.\n\n\nNarayanan, Arvind. 2022. “The Limits of the Quantitative Approach to Discrimination.” Speech."
  },
  {
    "objectID": "posts/midSemesterGoals/index.html",
    "href": "posts/midSemesterGoals/index.html",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Liz Rightmire\n\n\nIn this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.)\n\nalways\n\nHow often have you taken notes on the core readings ahead of the class period?\n\nalways\n\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called?\n\nalways\n\nHow many times have you actually presented the daily warm-up to your team?\n\nmaybe three times?\n\nHow many times have you asked your team for help while presenting the daily warm-up?\n\nnever\n\nHow often have you learned something new from a teammate’s presentation of the daily warm-up?\n\nalways\n\nHow often have you helped a teammate during the daily warm-up presentation?\n\nonce or twice\n\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help?\n\nonce\n\nHow often have you asked for or received help from your fellow students?\n\nmaybe three times\n\nHave you been regularly participating in a study group outside class?\n\nfor one or two blog posts\n\nHow often have you posted questions or answers in Slack?\n\nnever\n\n\n\n\n\n\nHow many blog posts have you submitted?\n\n4, and working on a 5th\n\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 2\nM: Revisions useful: 2\nR: Revisions encouraged: 0\nN: Incomplete: 0\n\nRoughly how many hours per week have you spent on this course outside of class?\n\nat least 10\n\n\n\n\n\n\nAt the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI hoped to complete at one blog post in each category, with special emphasis on Theory and Social Responsibility. I think that I am on track to achieving these goals, having completed the Women in Data Science essay and the Limitations to the Qualitative Approach essay, which fall under the Social Responsibility Category. I am currently working on the Perceptron blog post, which fits under Theory.\n\n\n\nFor each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nI struggled to quantify exactly how many blog posts I hoped to achieve, instead describing a desire to push myself in this class and be in the “ambitious” range of students. There’s danger in this, as it relies on comparison to others to determine my success. That being said, the stats Professor Phil posted in Slack last week do place me in the abitious range!\n\n\n\nI’ve been well-prepared for class each day – I always take notes on the readings, review the posted notes, and complete the warm-ups. I’ve always felt prepared to present the warm-ups even if I’m not chosen and I’ve been able to help some peers with their presentations!\nI haven’t had to miss any classes for illnesses and I feel as though I’ve been engaged and attentive in class. I try to contribute often in the small-group setting and be supportive of the presenter.\nWhen I’ve been selected to present, I’ve taken it seriously and tried my best to be consise, clear, and knowledgeable.\nI haven’t spoken much in the large-class setting. I also haven’t felt the need to attend office hours very frequently, but it may be a good for me to go more often to re-inforce what I’m leraning and ask questions about the theory.\n\n\n\nI look forward to brainstorming topics for the warm-up due Tuesday. I’ve already spoken with a few peers about working together, and I still aspire to provide technical, interpersonal, and presentation contributions.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nN/A\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nHaving completed 4 blog posts at the half-way point, I’d like to move away from comparing to other students to be the “ambitous range” and instead aim to complete two or three more blogs. I want to make sure I’m leaving time for the final project!\nMy original goal was to get 5 blogs to the “E” cateogry. With 2 blog posts in this category currently, I think it is reasonable to hope to get at least one more blog post to “no revisions suggested.”\nI initially hoped to be a leader in the study group I was working with, but with different students working on different blogs the group has somewhat dissolved, yet we still help each other with specific questions. Therefore, I will let this goal go.\nI think I’m safely on track for my other goals!\n\n\n\n\nTake 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI have been able to prioritize this class in a way that I’d hoped. I spend careful, intentional time preparing for each class and I am proud of the way I hold myself in the classroom. I have gained a lot of knowledge from each blog post I complete, and I am pleased with the progress I have made on them. I have developed a nuanced understanding of the field of machine learning, especially considering the work I’ve done in the Social Responsibility cateogry. I remain excited about the field of machine learning and I look forward to continuing to produce good work in this class!\n\n\n\n\nYou may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nN/A"
  },
  {
    "objectID": "posts/midSemesterGoals/index.html#the-data",
    "href": "posts/midSemesterGoals/index.html#the-data",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "In this section I’ll ask you to fill in some data. You don’t have to give precise numbers – approximate, conversational responses are fine. For example, when I ask “how often have you attended class,” good answers include “almost always,” “I’ve missed three times,” “about 75% of the time,” “not as often as I want,” etc.\n\n\n\nHow often have you attended class? (e.g. “almost always,” “I missed three times,” etc.)\n\nalways\n\nHow often have you taken notes on the core readings ahead of the class period?\n\nalways\n\nHow often have you been prepared to present the daily warm-up exercise to your team, even if you weren’t actually called?\n\nalways\n\nHow many times have you actually presented the daily warm-up to your team?\n\nmaybe three times?\n\nHow many times have you asked your team for help while presenting the daily warm-up?\n\nnever\n\nHow often have you learned something new from a teammate’s presentation of the daily warm-up?\n\nalways\n\nHow often have you helped a teammate during the daily warm-up presentation?\n\nonce or twice\n\n\n\n\n\n\nHow often have you attended Student Hours or Peer Help?\n\nonce\n\nHow often have you asked for or received help from your fellow students?\n\nmaybe three times\n\nHave you been regularly participating in a study group outside class?\n\nfor one or two blog posts\n\nHow often have you posted questions or answers in Slack?\n\nnever\n\n\n\n\n\n\nHow many blog posts have you submitted?\n\n4, and working on a 5th\n\nHow many of your submitted blog posts are at each of the following feedback stages?\n\nE: No revisions suggested: 2\nM: Revisions useful: 2\nR: Revisions encouraged: 0\nN: Incomplete: 0\n\nRoughly how many hours per week have you spent on this course outside of class?\n\nat least 10"
  },
  {
    "objectID": "posts/midSemesterGoals/index.html#what-youve-learned",
    "href": "posts/midSemesterGoals/index.html#what-youve-learned",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "At the beginning of the course, you may have expressed an interest in focusing a little extra on one or two of the following four categories:\n\nTheory: mathematical descriptions of frameworks and algorithms.\nImplementation: effective coding and use of tools in order to implement efficient machine learning algorithms.\nExperimentation: performing experiments to assess the performance of algorithms and clearly communicating about the results.\nSocial responsibility: critical analysis of sources of bias and harm in machine learning algorithms; theoretical formulations of fairness and bias\n\nDid you choose to focus on any of these categories? If so, what have you done in order to pursue your interest?\nI hoped to complete at one blog post in each category, with special emphasis on Theory and Social Responsibility. I think that I am on track to achieving these goals, having completed the Women in Data Science essay and the Limitations to the Qualitative Approach essay, which fall under the Social Responsibility Category. I am currently working on the Perceptron blog post, which fits under Theory."
  },
  {
    "objectID": "posts/midSemesterGoals/index.html#reflecting-on-goals",
    "href": "posts/midSemesterGoals/index.html#reflecting-on-goals",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "For each of the categories below, replace the “[your response here]” cell with 1-2 paragraphs in which you reflect on the following questions:\n\nIn what ways are you on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what you are doing in order to meet it.\nIn what ways are you not on track to meet your goals from the beginning of the course? Be specific: explain what the goal is and what gap you see between where you are and your goal.\nIf there’s any context you want to share about how you are faring relative to your goals, please do!\n\n\n\nI struggled to quantify exactly how many blog posts I hoped to achieve, instead describing a desire to push myself in this class and be in the “ambitious” range of students. There’s danger in this, as it relies on comparison to others to determine my success. That being said, the stats Professor Phil posted in Slack last week do place me in the abitious range!\n\n\n\nI’ve been well-prepared for class each day – I always take notes on the readings, review the posted notes, and complete the warm-ups. I’ve always felt prepared to present the warm-ups even if I’m not chosen and I’ve been able to help some peers with their presentations!\nI haven’t had to miss any classes for illnesses and I feel as though I’ve been engaged and attentive in class. I try to contribute often in the small-group setting and be supportive of the presenter.\nWhen I’ve been selected to present, I’ve taken it seriously and tried my best to be consise, clear, and knowledgeable.\nI haven’t spoken much in the large-class setting. I also haven’t felt the need to attend office hours very frequently, but it may be a good for me to go more often to re-inforce what I’m leraning and ask questions about the theory.\n\n\n\nI look forward to brainstorming topics for the warm-up due Tuesday. I’ve already spoken with a few peers about working together, and I still aspire to provide technical, interpersonal, and presentation contributions.\n\n\n\nIs there anything else that you want to share with me about what you have learned, how you have participated, or what you have achieved in CSCI 0451?\nN/A\n\n\n\nFrom your experience in CSCI 0451 and your other classes this semester, you may feel moved to make modifications to your goals. Are they still feasible? Too ambitious? Not ambitious enough? If you would like to revise any of your goals from your reflective goal-setting, you can do so below. For each goal you want to modify:\n\nClearly state what the goal was.\nClearly state how you’ve done on that goal so far.\nClearly state your proposed revised goal for the remainder of the course.\n\nHaving completed 4 blog posts at the half-way point, I’d like to move away from comparing to other students to be the “ambitous range” and instead aim to complete two or three more blogs. I want to make sure I’m leaving time for the final project!\nMy original goal was to get 5 blogs to the “E” cateogry. With 2 blog posts in this category currently, I think it is reasonable to hope to get at least one more blog post to “no revisions suggested.”\nI initially hoped to be a leader in the study group I was working with, but with different students working on different blogs the group has somewhat dissolved, yet we still help each other with specific questions. Therefore, I will let this goal go.\nI think I’m safely on track for my other goals!"
  },
  {
    "objectID": "posts/midSemesterGoals/index.html#grade-and-goals",
    "href": "posts/midSemesterGoals/index.html#grade-and-goals",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "Take 15 minutes to look back on your responses in each of the sections above. Then, state the letter grade that you feel reflects your learning, participation, and achievement in CSCI 0451 so far, and contextualize it against some of the soundbytes below.\n\n\nAn A sounds like:\n\n“I am very proud of my time in this course.”\n“I have grown significantly in multiple ways that matter to me.”\n“I am ready to take the theory, techniques, and ideas of this course into my future classes, projects, hobbies, or career.”\n\nA B sounds like:\n\n“I had some opportunities to learn more, overall I feel good about my time in this course.”\n“I am able to explain some new things or achieve new tasks.”\n“I can see a few ideas from this course that will be relevant for my future classes, projects, hobbies, or career.”\n\nA C sounds like:\n\n“I often made a good effort, but I missed many opportunities to get more out of my time in this course.”\n“I might be able to complete some new tasks related to the course content, but only with significant further guidance.”\n“I don’t see any ways to take the contents of this course into my future classes, projects, hobbies, or career.”\n\nYou might find that some of these soundbytes resonate and other’s don’t! Take some time, see what feels right, and don’t be afraid to celebrate your achievements.\n\nUpon reflection, I feel that my learning, participation, and achievement in CSCI 0451 (so far) are best reflected by a grade of A\n\n\nA way in which I resonate with the soundbytes for that grade above is…\n\nI have been able to prioritize this class in a way that I’d hoped. I spend careful, intentional time preparing for each class and I am proud of the way I hold myself in the classroom. I have gained a lot of knowledge from each blog post I complete, and I am pleased with the progress I have made on them. I have developed a nuanced understanding of the field of machine learning, especially considering the work I’ve done in the Social Responsibility cateogry. I remain excited about the field of machine learning and I look forward to continuing to produce good work in this class!"
  },
  {
    "objectID": "posts/midSemesterGoals/index.html#optional-how-to-improve",
    "href": "posts/midSemesterGoals/index.html#optional-how-to-improve",
    "title": "Mid-Course Reflection",
    "section": "",
    "text": "You may feel disappointed by your reflection. Sometimes we don’t achieve all our goals – it happens and it’s normal! If you are feeling disappointed by how you’ve learned, participated, or achieved in CSCI 0451, then feel free to write something about that below. Feel free to just write your feelings. If you have ideas for how to move forward, include those too! We’ll talk.\nN/A"
  },
  {
    "objectID": "posts/Perceptron/index.html",
    "href": "posts/Perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "Implementing the Perceptron Algorithm\nPerceptron.py\n\nAbstract\nThis blog is a deep dive into the perceptron algorithm – a supervised binary classification algorithm created in 1957. This algorithm begins with a randomly generated weight vector, and then updates the weight vectors using a random data point and a score generated from the point and the current weight vector. When the loss function quantifying the discrepency between predicted values of the data and actual values is 0, the algorithm terminates and the resulting weight vector describes the decision boundary between the classes.\nI begin by describing my implementation of a perceptron algorithm, and then investigate the algorithm’s effectiveness on three types of datasets: linearly separable, not linearly separable, and multi-dimensional. As expected, the algorithm is able to find the decision boundary between linearly separable classes, yet fails with not linearly separable classes. Finally, the five dimensional data I generate does not appear to be separable via a perceptron algorithm.\n\nimport torch\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n# load contents of perceptron.py\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer\n\n\n\nPerceptron Implementation\nYou can find my implementation of the perceptron algorithm at Perceptron.py.\nPerceptron.grad first computes the score for each data point in the feature matrix. The score is the feature matrix X matrix multiplied by the weight vector.\nIf the score is a different sign from the actual class label, the update is computed. Else, zeros are returned – this entry in the weight vector doesn’t need to be updated.\n\n\nExperiment 1: Linearly Separable\nWhen given linearly separable data, the perceptron algorithm should converge to a weight vector that describes a line separating the two classes of data. Let’s check that this is true for our implementation of the perceptron algorithm.\nFirst, generate linearly separable data\n\n# import some data\nplt.style.use('seaborn-v0_8-whitegrid')\n\ntorch.manual_seed(1234)\n\ndef perceptron_data(n_points = 96, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n\n    # convert y from {0, 1} to {-1, 1}\n    y = 2*y - 1\n\n    return X, y\n\nX, y = perceptron_data(n_points = 300, noise = 0.2)\n\ndef plot_perceptron_data(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [-1, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = perceptron_data()\nplot_perceptron_data(X, y, ax)\n\n\n\n\n\n\n\n\ncode modified from Prof Phil’s Notes\nIt is easy to imagine a decision boundary that would divide these two classes.\nNow, let’s run the perceptron algorithm. After just 3 iterations, the loss is 0 and a dividing line is discovered.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\n\nloss = 1.0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X, y) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nHow did this happen? Using a simple loop, the answer to this question can be visualized on side-by-side plots. The circled point is the one randomly chosen for the perceptron update, and the line updates from the dotted line to the solid line.\n\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\n\ntorch.manual_seed(1234567)\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\np.loss(X, y)\n\n# set up the figure\nplt.rcParams[\"figure.figsize\"] = (7, 5)\nfig, axarr = plt.subplots(2, 2, sharex = True, sharey = True)\nmarkers = [\"o\", \",\"]\nmarker_map = {-1 : 0, 1 : 1}\n\n# initialize for main loop\ncurrent_ax = 0\nloss = 1\nloss_vec = []\n\nwhile loss &gt; 0:\n    ax = axarr.ravel()[current_ax]\n\n    # save the old value of w for plotting later\n    old_w = torch.clone(p.w)\n\n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X[[i],:]\n    y_i = y[i]\n\n\n    # make an optimization step -- this is where the update actually happens\n    # now p.w is the new value \n    local_loss = opt.step(x_i, y_i)\n\n    # if a change was made, plot the old and new decision boundaries\n    # also add the new loss to loss_vec for plotting below\n    if local_loss &gt; 0:\n        plot_perceptron_data(X, y, ax)\n        draw_line(old_w, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n        loss = p.loss(X, y).item()\n        loss_vec.append(loss)\n        draw_line(p.w, x_min = -1, x_max = 2, ax = ax, color = \"black\")\n        ax.scatter(X[i,0],X[i,1], color = \"black\", facecolors = \"none\", edgecolors = \"black\", marker = markers[marker_map[y[i].item()]])\n        # draw_line(w, -10, 10, ax, color = \"black\")\n        ax.set_title(f\"loss = {loss:.3f}\")\n        ax.set(xlim = (-1, 2), ylim = (-1, 2))\n\n        if(current_ax &lt; 3):\n            current_ax += 1\n        else:\n            break\nplt.tight_layout()\n\n\n\n\n\n\n\n\nvisualization code modified from Prof Phil’s Notes\n\n\nExperiment 2: Not Linearly Separable\nLet’s generate some data that isn’t linearly separable by increasing the noise of our original data\n\nX_noise, y_noise = perceptron_data(n_points = 300, noise = 0.7)\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X_noise, y_noise, ax)\n\n\n\n\n\n\n\n\nNow, let’s run the perceptron algorithm\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss = 1.0\niteration = 0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X_noise.size()[0]\n\nwhile loss &gt; 0 and iteration &lt; 1000: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_noise, y_noise) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X_noise[[i],:]\n    y_i = y_noise[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration = iteration + 1\n\nfinal_weights = opt.model.w\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nEven with 1,000 perceptron updates, the algorithm does not terminate because the loss will never reach 0!\nWhat dividing line does the perceptron algorithm produce, 1,000 updates later?\n\nfig, ax = plt.subplots(1, 1)\nplot_perceptron_data(X_noise, y_noise, ax)\ndraw_line(final_weights, x_min = -2, x_max = 3, ax = ax, color = \"black\", linestyle = \"dashed\")\n\n\n\n\n\n\n\n\nHmm, this isn’t even a very good separator. Obviously, the perceptron algorithm performs poorly on data that isn’t perfect linearly separable.\n\n\nExperiment 3: Multi-Dimensional Data\nThe perceptron algorithm should work on data that is more than 2-dimensional.\nFirst, geneate some five-dimensional data to experiment with.\n\ndef five_d_data(n_points=100, noise=0.2):\n    # Generate random points in 5D space\n    X = torch.randn(n_points, 5)\n    y = (X[:, 0] + X[:, 1] * X[:, 2] &gt; 0).float()\n\n    return X, y\n\nX_5d, y_5d = five_d_data()\n\nNow, run the perceptron algorithm.\n\n# instantiate a model and an optimizer\np = Perceptron() \nopt = PerceptronOptimizer(p)\nloss = 1.0\niteration = 0\n\n# for keeping track of loss values\nloss_vec = []\n\nn = X_5d.size()[0]\n\nwhile loss &gt; 0 and iteration &lt; 3000: # dangerous -- only terminates if data is linearly separable\n    \n    # not part of the update: just for tracking our progress    \n    loss = p.loss(X_5d, y_5d) \n    loss_vec.append(loss)\n    \n    # pick a random data point\n    i = torch.randint(n, size = (1,))\n    x_i = X_5d[[i],:]\n    y_i = y_5d[i]\n    \n    # perform a perceptron update using the random data point\n    opt.step(x_i, y_i)\n\n    iteration = iteration + 1\n\n\nplt.plot(loss_vec, color = \"slategrey\")\nplt.scatter(torch.arange(len(loss_vec)), loss_vec, color = \"slategrey\")\nlabs = plt.gca().set(xlabel = \"Perceptron Iteration (Updates Only)\", ylabel = \"loss\")\n\n\n\n\n\n\n\n\nEven after 3,000 iterations, the loss stays fairly high – around 0.3. The loss does not appear to be consistently decreasing with each update. This leads me to believe that the data I created isn’t linearly separable.\n\n\nMinibatch Perceptron Experiments\nI edited my perceptron.py file to be compatable with the mini-batch perceptron algorithm, which computes an update using k points at once rathar than a single point.\nNow, I will conduct experiments to explore how minibatch perceptron performs in different scenarios. First, set k = 1.\n\ndef plot_loss(loss):\n    plt.style.use('seaborn-v0_8-whitegrid')\n    plt.figure(figsize=(10, 6))\n\n\n    plt.plot(loss, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss)), loss, color = \"slategrey\", s=5)\n    labs = plt.gca().set(xlabel = \"Perceptron Iteration\", ylabel = \"loss\")\n    \n    plt.title(f\"Final loss: {loss[len(loss)-1]:.3f}\")\n\n#[CITE]\n\n\ntorch.manual_seed(88)\n\nX, y = perceptron_data()\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\nloss = 1.0\n\n# set k to 1\nk = 1\n\n# keep track of loss\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    # random submatrix of feature matrix passed to PerceptronOptimizer.step()\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\n\nplot_loss(loss_vec)\n\n\n\n\n\n\n\n\nWith k = 1, minibatch performs similarly to the regular perceptron. The loss tends to decrease until reaching 0.\nLet’s try k = 10.\n\ntorch.manual_seed(88)\n\nX, y = perceptron_data()\n\n# initialize a perceptron \np = Perceptron()\nopt = PerceptronOptimizer(p)\nloss = 1.0\n\n# set k = 10\nk = 10\n\n# keep track of loss\nloss_vec = []\n\nn = X.size()[0]\n\nwhile loss &gt; 0:\n    # random submatrix of feature matrix passed to PerceptronOptimizer.step()\n    ix = torch.randperm(X.size(0))[:k]\n    x_i = X[ix,:]\n    y_i = y[ix]\n    \n    opt.step(x_i, y_i)\n    \n    loss = p.loss(X, y).item()\n    loss_vec.append(loss)\n\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\n\n\n\nConclusion\nThe update in each iteration of the perceptron algorithm is computed as X * w, the product between a single row of matrix X and the weight vector w. Each row in X has p entries for the p features, so this process is O(p).\nI was able to prove that the perceptron algorithm, though simple, is both effective and efficient at finding a linear classifier for data of any dimension. I wrote my first implementation of an algorithm and gained a deeper understanding of its inner workings. This knowledge will translate well when I study more complicated machine learning algorithms."
  },
  {
    "objectID": "posts/LogisticRegression/index.html",
    "href": "posts/LogisticRegression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "Logistic Regression Implementation\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\nSource code for Logistc.py\n\nAbstract\nLogistic Regression is a popular machine learning algorithm most commonly used in classification problems. In this blog post, I will create an object-oriented implementation of Logistic Regression. Then, I will perform a series of experiments to test the efficacy of Logistic Regression in different scenarios. First I consider vanilla gradient descent, in which the momentum term \\(\\beta\\) is set to 0. Next I adjust \\(\\beta\\) to a positive, nonzero value to see how introducing momentum into the algorithm allows for faster convergence. Finally, I investigate overfitting in Logistic Regression, seeing how a model can have a training accuracy of 100%, yet only 86% accuracy on a separate, testing dataset.\n\n\nExperiments\n\n\nVanilla Gradient Descent\nTo begin, I will implement “vanilla” gradient descent, in which the momentum term, \\(\\beta\\), is 0. Referring to the equation below, setting \\(\\beta\\) to 0 cancels out the last term. The step in the gradient descent algorithm is computed as \\(\\alpha\\) * the gradient of the loss function.\n$_{k+1} _k - L(_k) + (k - {k-1}) $\nFirst, define functions to generate and plot our classification data, and one to draw the decision boundary we’ll find next.\n\n# generate data for classification problem\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):   \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    return X, y\n\n\n# function to plot classification data\ndef plotter(X, y, ax):\n    assert X.shape[1] == 3, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"*\" , \"o\"]\n    colors = [\"red\", \"blue\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"jet\", vmin = -2, vmax = 2, alpha = 0.5, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\n\n# function to draw decision boundary\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nPlot the classification data we’ll work with\n\nX, y = classification_data(noise = 0.2)\nfig, ax = plt.subplots(1,1)\nplotter(X,y,ax)\n\n\n\n\n\n\n\n\nWe have two distinct classes of points, and our goal is to find a decision boundary between them.\nWe can now train our logistic model and follow our progress by plotting the loss vs. the current iteration. We’ll save the final weight vector before the loop terminates to plot the decision boundary.\n\n# initialize an instance of Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\n\n# training loop\nfor index in range(1500):\n    # for vanilla gradient descent, alpha must be sufficiently small and beta must be 0\n    opt.step(X, y, alpha = 0.3, beta = 0)\n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n# save final weights for graphing decision boundary\nfinal_weights = LR.w\n    \ndef plot_loss(loss_vec, color):\n    plt.plot(loss_vec, color = \"slategrey\")\n    plt.scatter(torch.arange(len(loss_vec)), loss_vec, color = color, s = 5)\n    plt.gca().set(xlabel = \"Iteration\", ylabel = \"loss\")\n    plt.title(\"Loss Minimization\")\n\nplot_loss(loss_vec, \"green\")\n\n\n\n\n\n\n\n\n\n# plot decision boundary\nfig, ax = plt.subplots(1,1)\nplotter(X, y, ax)\ndraw_line(final_weights, x_min = -1, x_max = 2, ax = ax, color = \"black\", linestyle = \"dashed\")\n\n\n\n\n\n\n\n\nThe decision boundary found using vanilla gradient descent is a perfect match!\n\n\nBenefits of Momentum\nVanilla gradient descent was very effective, but we can do better. Adjusting the momentum term \\(\\beta\\) to a value other than 0 allows gradient descent to converge in fewer iterations.\n$_{k+1} _k - L(_k) + (k - {k-1}) $\nNow that \\(\\beta\\) is not 0, The final term, or momentum term, is factored into the step calculation. The size of the difference between the current weights and previous iteration’s weights have an impact on the size of the step.\nLet’s prove this to ourselves.\nTrain a logistic regression model with \\(\\beta\\) = 0.9\n\n# initialize an instance of Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\nloss_vec_momentum = []\n\n# training loop -- beta = 0\nfor index in range(1500):\n    opt.step(X, y, alpha = 0.3, beta = 0)\n    loss = LR.loss(X, y).item()\n    loss_vec.append(loss)\n\n# training loop -- momentum beta = 0.9\nfor index in range(1500):\n    # adjust momentum term\n    opt.step(X, y, alpha = 0.3, beta = 0.9)\n    loss = LR.loss(X, y).item()\n    loss_vec_momentum.append(loss)\n\n# save final weights for graphing decision boundary\nfinal_weights = LR.w\n\nplot_loss(loss_vec_momentum, \"red\")\nplot_loss(loss_vec, \"green\")\n\n\n\n\n\n\n\n\nThe green line represents vanilla gradient descent, whereas the red line is gradient descent with momentum. With momentum, we see significant improvements – the initial loss is much lower, and the loss stays lower than vanilla gradient descent for each iteration up until 1,500.\n\n\nOverfitting\nLogistic Regression models have the tendency to overfit. Let’s explore this pattern.\nGenerate train and test data where p_dim &gt; n_points.\n\nX_train, y_train = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 50, noise = 0.5, p_dims = 100)\n\nFit the model on the training data\n\n# initialize an instance of Logistic Regression \nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\n# initialize for main loop\nloss_vec = []\n\n# training loop\nfor index in range(100):\n    opt.step(X_train, y_train, alpha = 0.3, beta = 0)\n\nfinal_weights = LR.w\n\nWe obtain an accuracy of 100% on the training data. This should raise concern that we may have oveffit!\n\n# Accuracy on training data\n(1.0*(LR.predict(X_train) == y_train)).mean()\n\ntensor(1.)\n\n\nIndeed, accuracy is much lower – 0.84 – on the testing data.\n\n# Accuracy on testing data\n(1.0*(LR.predict(X_test) == y_test)).mean()\n\ntensor(0.9000)\n\n\n\n\nDiscussion\nIn this blog post, I set out to perform a deeper dive into a stalwart in the machine larning world: logistic regression. Instead of relying on sklearn’s implementation as I have done in the past, I built my own logistic regression model and tested it in a variety of scenarios. The intricacies of the gradient and loss equations became more concrete as I toyed with tensors to produce the desired result.\nBeginning with vanilla gradient descent, I observed how the loss fell to zero and the decision bounary was stored in weights. Then, I dove into the theory behind momentum (how and why does it work?) and set \\(\\beta\\) to 0.9 such that the momentum term of the step function would be at play. Finally, I saw how logistic regression can overfit – something I’ll be wary of in future projects."
  }
]