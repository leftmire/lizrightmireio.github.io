{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: Limits of the Quantitative Approach to Bias and Fairness\n",
    "author: Liz Rightmire\n",
    "date: '2024-03-14'\n",
    "image: \"image.jpg\"\n",
    "description: \"An essay considering and critiquing methods used to ensure fair algorithms\"\n",
    "format: html\n",
    "bibliography: refs.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits to the Quantitative Approach to Bias and Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We live in the era of “Big Data.” Information regarding our individual movements and tendencies is constantly being collected via digital devices, sensors, and online platforms. This data takes various forms, be it traditional databases, semi-structured formats like JSON, and unstructured data from sources like social media. Perhaps unsurprisingly, the pace with which this data is processed is constantly increasing -- nearing real-time.  Endless data is publicly available; massive datasets can be downloaded from portals or through APIs for any analytics use. Terabytes of data are funneled into machine learning algorithms which make decisions and offer recommendations for the common person, whether or not they’re aware of it. Companies, policymakers, and computer scientists alike rave about the efficiency and seemingly endless possibilities provided by big data and big data analytics.\n",
    "\n",
    "As algorithms started being used for decisions with significant outcomes, critics began to recognize the potential for these algorithms to become biased against individuals of certain identity groups, causing additional harm. Famously, algorithms used by the police force to predict where crimes may occur and algorithms used by the justice department to predict the probability of a defendant re-committing a crime were proven to be rampantly biased against people of color. Individuals concerned with the ethical implications of data-driven decisions sparked a movement countering Big Data, pushing for metrics of fairness in algorithms. Now, a framework for verifying fairness is common practice, relying on various quantitative methods. Researchers can use statistical definitions of fairness as a crutch to support their algorithmic findings, but it still remains uncommon (@kordzadeh2022algorithmic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions of Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three main quantitative definitions of fairness. First is error rate parity, in which all groups experience the same false negative rate and false positive rate. This means that whether a person is a member of Group A or Group B, the algorithm is equally likely to make a mistake. The next is acceptance rate parity, in which an algorithm equalizes acceptance rates across all groups. The outcome of an algorithm should be independent of group membership. Finally is sufficiency, in which the probability of seeing a positive outcome given a positive prediction is the same for all subgroups. Similarly, the probability of seeing a negative outcome given a negative prediction must also be the same for all subgroups (@barocas2023fairness).\n",
    "\n",
    "Now, consider the moral standpoint, which also contains three “views” from which one can consider fairness: a narrow, middle, and broad view. In the narrow view, people similar with respect to a task should be treated similarly. Comparison should be between all people as individuals, not between members of specific groups. The middle view begs decision-makers to uphold an obligation to avoid perpetuating injustice; they must treat seemingly dissimilar people similarly when the causes of those differences are themselves problematic. Finally, the broad view of equality hopes that people with similar abilities and ambitions could achieve similar successes, despite inevitable inequities. This view isn’t really about fairness in decision-making, it’s about the design of society’s basic institutions, with the goal of preventing unjust inequalities from arising in the first place (@barocas2023fairness). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Utility of Quantitative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of where quantitative and moral definitions of fairness are effectively leveraged to critique an algorithm appears in a study performed by Ali et al. titled “Discrimination through Optimization: How Facebook's Ad Delivery Can Lead to Biased Outcomes.” In this study, the authors consider Facebook’s ad targeting algorithm, hoping to determine if it targets job advertisements differently for men and women. Facebook states, “we try to show people the ads that are most pertinent to them,” In doing this, the researchers wonder, does Facebook make gender or racial generalizations? The team created generic job posting advertisements for eleven different roles: AI developer, doctor, janitor, lawyer, lumberjack, nurse, preschool teacher, restaurant cashier, secretary, supermarket clerk, and taxi driver. The advertisements were submitted to Facebook and the researchers paid for them to be “live” for 24 hours. Then, the research team collected ad delivery data and broke the results along gender, age, and race lines. They observed dramatic differences in ad delivery in different racial and gender groups. “Our ads for positions in the lumber industry deliver to over 90% men and to over 70% white users in aggregate, while our ads for janitors deliver to over 65% women and over 75% black users in aggregate” (@ali2019discrimination). \n",
    "\n",
    "Here, the researchers proved that Facebook’s algorithm did not uphold the acceptance rate parity. The probability of receiving a lumber industry advertisement should be independent of gender, and the probability of receiving a janitorial advertisement should be independent from race, yet neither of these is the case. As far as non-technical definitions of fairness, the research team claimed that the algorithm was unfair considering the middle view. Facebook has an obligation to treat dissimilar people (men and women) similarly when the causes of the dissimilarities are themselves problematic. In this case, it is problematic to assume that men are more interested in lumberjack and AI roles than women, even if it is the case that more men click on these advertisements than women. Therefore, Facebook must consider adjusting its advertising scheme so as not to perpetuate gender stereotypes in labor. This excellent study acts as an optimistic example of the utilitarianism of quantitative metrics to shine a light on unintentional algorithmic bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note of Caution: Arvind Naryanan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these quantitative measures of fairness aren’t enough to claim an algorithm’s innocence, claims Arvind Narayanan, a computer scientist and professor at Princeton University. On October 11th, 2022, he gave a talk titled “The Limits of the Quantitative Approach to Discrimination,” which provides a timely and fresh criticism of quantitative measures of fairness. His central claim is: “Currently quantitative methods are primarily used to justify the status quo. I would argue that they do more harm than good” (@narayanan2022limits).\n",
    "\n",
    "Narayanan begins his speech by explaining that “all models are wrong, but some models are useful” (@narayanan2022limits). No machine learning model makes the right prediction every time; the simplification of trends necessary to fit a clean model introduces bias. In such generalizations, models feast on the status quo, engraining generalities and failing to treat them as problematic. However, we know that the way our world operates is extremely problematic; our reality is replete with discrimination. Therefore, Narayanan believes that “data aren’t inert and objective. They are political, and produced towards certain ends” (@narayanan2022limits). When data are collected for a certain purpose, there is inevitable bias baked into them. This notion is summarized well by Narayanan and co-authors Barocas and Hardt in their book *Fairness and Machine Learning: Limitations and Opportunities*. In the introduction, the authors explain that “the state of the world is reduced to a set of rows, columns, and values in the dataset. It’s a messy process, because the real world is messy. The term measurement is misleading, evoking an image of a dispassionate scientist recording what she observes, yet …it requires subjective human decisions.” (@barocas2023fairness). There’s a popular notion that data doesn’t lie and a conclusion drawn from data must be fact. However, Narayanan’s concept of data explains it not as truth but as a concentrated soup of the biases, discriminations, and unequal opportunities present in the real world. To combat this characteristic of data, Narayanan says, “we should be spending most of our time on curating and interrogating datasets before ever searching for statistical significance or fitting a model… it is important to look behind the facade of numbers to understand the hidden assumptions and politics of datasets” (@narayanan2022limits).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks to the Quantitative Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is truth to Narayanan’s claim; it isn’t challenging to find studies where quantitative methods are used to prove an algorithm’s fairness when in reality, it is biased. In a study titled Gender Earning Gap in the Gig Economy, Cook et al. set out to investigate if Uber’s algorithm is to blame for a 7% gender pay gap among rideshare drivers. The authors hone in on driver data from the metropolitan area of Chicago, and are able to find a statistical significance in the difference in means between the driving speeds of men and women and the time of day they choose to work. They also determine that men stay on the platform for a longer amount of time, causing them to be more experienced. Finally, they find a difference between neighborhoods where male and female drivers choose to drive. Therefore, the authors reason that the pay gap is due to these differences alone and not due to any bias in Uber’s algorithm. Yes, the results of the quantitative method of choice – statistical tests – is valid, but this is an incredibly simplistic way to view the issue. The authors would have done well to consider the middle view of equality, in which decision-makers have an obligation to avoid perpetuating injustice. There’s obvious injustice here: women are likely to drive in a smaller subset of neighborhoods and during daylight hours when wages are less because of the history of assault and mistreatment towards women, typically occurring at late hours and disproportionally in certain areas. Looking deeper into the study, it is revealed that many of the authors are Uber employees. It isn’t surprising, then, that they choose to leverage quantitative methods in a dishonest way. (@cook2021gender)\n",
    "\n",
    "This study is a prime example of \"Big Dick Data,\" as defined by D'ignazio, Catherine and Klein in their book *Data Feminism*. \"Big Dick Data is used to describe big data projects characterized by patriarchial, cis-masculinist, totalizing fantasizes of world domination as enacted through data capture analysis\" (@d2023data). Big Dick data projects are dangerous because they ignore context and inflate their technical and scientific capabilities. The authors of *Data Feminism* highlight the importance of asking questions about the social, cultural, historical, instutional, and material conditions around how data was collected, a process Ali et al. failed to do -- miserably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Narayanan began his speech by saying, “I hope that this talk goes some way towards busting the myth that numbers don’t lie” (@narayanan2022limits).  For me, at least, he succeeded in weakening my loyalty to data as capital-T Truth. I’ve realized that there is no way to collect a completely neutral, unbiased dataset, and therefore models always reflect the messy world we live in. You can just about always use quantitative methods to prove what you’re looking for, and that’s why a choice of null hypothesis is so important. In Ali et. al's analysis of Uber’s algorithm, they set out to prove that gender bias wasn’t present, and unsurprisingly, they were able to prove such a thing using valid statistical claims. \n",
    "\n",
    "However, Narayanan goes so far as to say that quantitative methods do more harm than good and are too commonly used to justify racism. While I appreciate his stance, I still see the utility in quantitative methods; they can effectively be used to prove an algorithm to be racist or biased.  It is true that there are so many quantitative definitions of fairness, meaning that under one definition, an algorithm can be proven to be biased, but under another, it can be proven to be fair.  To bolster quantitative methods in a responsible, impactful way, we must not rely on just one quantitative definition of fairness and instead use them in combination with each other. Finally, we can’t just rely on numbers; it is important to bring the narrow, middle, and broad views of ethical fairness into every conversation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
