[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nMy name is Liz Rightmire\nI’m a computer science student at Middlebury College\nThis blog holds the work I’ve done in CSCI0451: Machine Learning"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Liz’s CSCI 0451 Blog",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\n\n\n\nSelecting features and model for accurate penguin species classifier\n\n\n\n\n\nFeb 20, 2024\n\n\nLiz Rightmire\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ClassifyingPalmerPenguins/index.html",
    "href": "posts/ClassifyingPalmerPenguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "CSCI 0451\n\nClassifying Palmer Penguins\n\n\n\ncite: https://education.rstudio.com/blog/2020/07/palmerpenguins-cran/\n\n\nThe Palmer Penguins dataset is a public dataset frequently used within the educational data science community. It contains Dr. Kristen Gorman and the Long Term Ecological Research Network’s observations of hundreds of antartic penguins belonging to 3 species groups: Adelie, Gentoo, and Chinstrap.\nThis blog aims to accomplish three goals:\n\nConduct exploration of the Palmer Penguins dataset\nMethodically select 3 features and a model type that produces 100% penguin species classification accuracy on test data\nEvaluate chosen model by analyzing decision regions and a confusion matrix\n\n\nRandom Exploration\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain[\"Species\"] = train[\"Species\"].str.split().str.get(0)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\nThe training data contains 18 observations about 275 penguin subjects. Let’s clean this data by dropping irrelevent columns and one-hot encoding the qualitative observations.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\nVisualizations\n\nimport seaborn as sns\nsns.set_palette(\"husl\", 3)\n\n# visualization 1: flipper length frequency by species\nsns.histplot(data = train, x = \"Flipper Length (mm)\", hue = \"Species\", bins = 25, edgecolor = 'grey')\n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\n\n\n\n\n\n\n\nThis histogram shows the distribution of the penguins’ flipper lengths. The bars are colored based on penguin species, so the shape of the distributions allow the viewer to infer the mean, medians, and modes for each species. There is a trend in flipper length based on species: Gentoo penguins tend to have the largest flippers, and Adelie and Chinstrap penguins have smaller ones. This indicates that flipper length may be a helpful feature to use in classification, as it effectively identifies the Gentoo penguins from the rest of the species.\n\n# visualization 2: body mass vs. culmen length\nplot1 = sns.scatterplot(train, x = \"Body Mass (g)\", y = \"Culmen Length (mm)\", hue = \"Species\")\n\n\n\n\n\n\n\n\nAfter considering multiple combinations of qualitative features in scatterplots, body mass and culmen length proved to show clear groupings of penguin species. Therefore, these two features are likely to be effective in a classification model. That being said, overlap does exist, especially between Adelie and Gentoo penguins.\n\n# summary table\ntable = train.groupby(['Species', 'Island']).size()\ntable\n\nSpecies    Island   \nAdelie     Biscoe       33\n           Dream        45\n           Torgersen    42\nChinstrap  Dream        57\nGentoo     Biscoe       98\ndtype: int64\n\n\nThis summary table shows the numbers of penguins present on each island. Every penguin on Torgersen island is an Adelie penguin, and both Gentoo and Chinstrap penguins can only be found on one island. These clear trends cause me to consider island location as a potential feature in a classification model.\n\n\n\ncite: https://en.ac-illust.com/clip-art/22518802/illustration-of-a-cute-penguins-playing-a-computer\n\n\n\n\nChoosing Features\nBecause there were only 5 qualitative and 6 quantitative feature in the cleaned dataset, I chose to perform an exhaustive search to determine the most effective features and model for species classification. For each combination of 2 quantitative and 1 qualitative feature, I fit 4 models: Logistic Regression, Decision Tree, Random Forest, and SVM. Cross validation was performed to guard against overfitting as follows:\n\nLogistic Regression: recorded average accuracy of 5 rounds, each round with a random 20% of data used for testing\nDecision Tree and Random Forest: performed grid search of varrying max_depth values, recorded highest accuracy achieved\nSVM: performed grid search of varrying gamma values, recorded highest accuracy achieved\n\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.set_option('max_colwidth', 10000)\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island\", 'Stage_Adult, 1 Egg Stage']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)', 'Delta 15 N (o/oo)', 'Delta 13 C (o/oo)' ]\n\ncolumns = ['features', 'model', 'score']\n\nscore_df = pd.DataFrame(columns = columns)\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair)\n\n    # Logistic Regression\n    LR = LogisticRegression(max_iter = 20000000000)\n    LR.fit(X_train[cols], y_train)  \n    LRscore = cross_val_score(LR, X_train[cols], y_train, cv = 5).mean()\n    score_df.loc[len(score_df.index)] = [cols, 'Logistic', LRscore]  \n\n    # Decision Tree\n    DTC = DecisionTreeClassifier()\n    param_grid = {'max_depth': [11,2,3,4,5,6,7,8,9,10, None]}\n    grid_search = GridSearchCV(DTC, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    DTCscore = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, 'Decision Tree', DTCscore]  \n\n    # Random Forest\n    randomforest = RandomForestClassifier()\n    grid_search = GridSearchCV(randomforest, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    RF_score = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, 'Random Forest', RF_score]\n\n    # SVM\n    param_grid = {'gamma': 10.0**np.arange(-5, 5)}\n    SVC_model = SVC()\n    grid_search = GridSearchCV(SVC_model, param_grid, cv = 5)\n    grid_search.fit(X_train[cols], y_train)\n    SVMscore = grid_search.best_score_\n    score_df.loc[len(score_df.index)] = [cols, \"SVM\", SVMscore]\n\nscore_df.sort_values(by='score', ascending=False).head(10)\n\n\n\n\n\n\n\n\nfeatures\nmodel\nscore\n\n\n\n\n60\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nLogistic\n0.988311\n\n\n62\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nRandom Forest\n0.988311\n\n\n66\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Flipper Length (mm)]\nRandom Forest\n0.984465\n\n\n126\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Flipper Length (mm)]\nRandom Forest\n0.984389\n\n\n122\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\nRandom Forest\n0.984389\n\n\n120\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Culmen Depth (mm)]\nLogistic\n0.984389\n\n\n166\n[Island_Biscoe, Island_Dream, Island_Torgersen, Flipper Length (mm), Delta 13 C (o/oo)]\nRandom Forest\n0.984314\n\n\n138\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Delta 13 C (o/oo)]\nRandom Forest\n0.980543\n\n\n63\n[Sex_FEMALE, Sex_MALE, Culmen Length (mm), Culmen Depth (mm)]\nSVM\n0.980543\n\n\n134\n[Island_Biscoe, Island_Dream, Island_Torgersen, Culmen Length (mm), Delta 15 N (o/oo)]\nRandom Forest\n0.976621\n\n\n\n\n\n\n\nFor each model, the features, model type, and highest possible score was added to a dataframe. Sorting this dataframe by score revealed that Logistic Regression with Sex, Culmen Length and Culmen Depth as features produced the best classification.\n\n\n\ncitation: https://www.dreamstime.com/stock-illustration-d-penguin-teaches-math-render-numbers-image45736629\n\n\n\n\nEvaluate Chosen Model\nTo truly evaluate our model, we must evaluate how it performs on unseen testing data.\n\n# train with optimal features and model\nLR = LogisticRegression()\noptimal_features = ['Culmen Length (mm)', 'Culmen Depth (mm)','Sex_FEMALE', 'Sex_MALE']\nLR.fit(X_train[optimal_features], y_train)  \n\n/Users/lizrightmire/anaconda3/envs/ml-0451/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\ntest[\"Species\"] = test[\"Species\"].str.split().str.get(0)\n\n#test\nX_test, y_test = prepare_data(test)\nLR.score(X_test[optimal_features], y_test)\n\n1.0\n\n\nA score of 1.0 indicates that 100% of the penguins in the testing dataset were correctly classified by our model. Yippie!\nStepping back a little, let’s consider: does this make sense? Should we be able to determine a penguin species based on its sex, culmen length, and culmen depth?\nYes, it seems logical that different species of penguins have different culmen dimensions. I am also not surprised that sex is an important qualitative feature to consider, as penguin size, and consequently beak dimensions, vary based on sex. For example, female Gentoo penguins may have similar culmen lengths to a male Adelie, so sex is required to determine species.\nTo be sure, let’s create a scatterplot of these 3 features and look at the decision regions produced by our logistic algorithm.\n\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nFor the training data:\n\n# training data\nplot_regions(LR, X_train[optimal_features], y_train)\n\n\n\n\n\n\n\n\nFor the testing data:\n\n# testing data\nplot_regions(LR, X_test[optimal_features], y_test)\n\n\n\n\n\n\n\n\nYes, separating penguins by sex creates very clear clusters of points by species in the testing data, which transfers perfectly to the points in the testing data.\nAnother way to evaluate would be to look at a confusion matrix for this model\n\nfrom sklearn.metrics import confusion_matrix\n\n#actual\n#predicted\n\ny_test_pred = LR.predict(X_test[optimal_features]) # guesses for each data\n\ncm = confusion_matrix(y_test, y_test_pred)\n\nfor i in range(3):\n    for j in range(3):\n        print(f\"There were {cm[i,j]} {le.classes_[i]} penguin(s) who were classified as {le.classes_[j]}.\")\n\nThere were 31 Adelie penguin(s) who were classified as Adelie.\nThere were 0 Adelie penguin(s) who were classified as Chinstrap.\nThere were 0 Adelie penguin(s) who were classified as Gentoo.\nThere were 0 Chinstrap penguin(s) who were classified as Adelie.\nThere were 11 Chinstrap penguin(s) who were classified as Chinstrap.\nThere were 0 Chinstrap penguin(s) who were classified as Gentoo.\nThere were 0 Gentoo penguin(s) who were classified as Adelie.\nThere were 0 Gentoo penguin(s) who were classified as Chinstrap.\nThere were 26 Gentoo penguin(s) who were classified as Gentoo.\n\n\nZero penguins were mis-classified. This makes sense because we got our logistic regression model had an accuracy of 1.0!\nThe plot below is another way to visualize the confusion matrix. For an accuracy of 1.0, we would expect zeros in every box execept for those on the diagonal, meaning that zero penguins were misclassified.\n\n# plot confusion matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\ncm_df = pd.DataFrame((cm), index = ['Gentoo','Chinstrap ','Adelie'], columns = ['Gentoo','Chinstrap','Adelie'])\ndisplay_labels = [\"Adelie\", \"Chinstrap\", \"Gentoo\"]\nConfusionMatrixDisplay(cm, display_labels = display_labels).plot(\n    include_values=True)"
  }
]